Prerequisites and Course Overview

Hi, my name is Janani Ravi, and I welcome you to this course on understanding the foundations of TensorFlow. 

As the amount of data recorded in this world explodes in size, human beings' ability to find patterns in this data is steadily diminishing. This is when we turn to machine learning. 

TensorFlow as a tool was open-sourced just in November 2015. So it's not been very long, but it's rapidly gained acceptance as a great library for numerical computation on a very large scale that machine learning demands. 

TensorFlow can be applied to all kinds of machine learning algorithms, but it's particularly suitable for deep learning with neural networks. 

This module will introduce you to TensorFlow and its basic building blocks so you create a strong foundation when you move on to more complex problems. This module assumes that you know nothing about machine learning. 

You'll get introduced not just to machine learning but also to what is deep learning and what part neural networks play within it. 


We'll get kickstarted with using TensorFlow in earnest and we'll understand why TensorFlow is slowly becoming the default library that people turn to for machine learning. It's been around only for about a year and a half in the public domain, but it has gained wide acceptance. We'll get started with TensorFlow by installing and setting it up on your local machine. What do you need to know before you can get started with this course so you can make the most of it? Turns out, very little. You need to be familiar with the command line on a Mac, Linux, or a Windows machine. You need to be comfortable writing code at a basic to intermediate level in Python, and really, that's it. All the building blocks and foundations of machine learning and TensorFlow will be covered in this course. We'll install the latest version of TensorFlow in this module. You need a compatible version of Python pre-installed on your machine. A compatible version is version 2. 7 or any Python version above 3. One caveat for Windows users. TensorFlow only works with Python versions 3. 5 and above for Windows. And the last bit is kind of obvious. You need a Mac, Linux, or Windows machine on which you can install TensorFlow. This course is a foundational course, which means we'll focus on what is going on in TensorFlow and how that applies to the world of machine learning. 

The first module is an introductory module, an introduction to machine learning, deep learning, a little bit of neural networks, and to TensorFlow, of course. 

The next module focuses on the basics of TensorFlow, how a program is represented in the real world. It's modeled using a computation graph. We'll talk about tensors, sessions, and how to visualize your program in TensorBoard. 

The third module covers the fundamentals of TensorFlow. We'll understand what placeholders are, where they are used, what makes variables useful, and how we use the feed dictionary, amongst other things. 

The fourth module builds on the core concepts that you've learned so far, and you'll apply them by working with images. You'll see how images are represented in TensorFlow, RGB as well as grayscale images and the common operations that you can perform with images. And finally, in the last module of this course, you'll use TensorFlow for a real machine learning algorithm. You'll identify hand-written digits in the MNIST dataset using the nearest neighbors algorithm.





Traditional ML Algorithms

TensorFlow is primarily used for machine learning, which means we need to understand what exactly machine learning is from first principles. These are programs which are intelligently written and capable of dealing with huge datasets. We collect so much data, and so much of it is junk. But within this machine learning algorithms are able to find patterns and extract information that is useful for making decisions. The objective of every machine learning algorithm is to ease decision-making by taking into account lots of data. A machine learning algorithm is one that's able to learn from data, make predictions, course correct, and fix itself if its predictions are wrong. A very standard example of machine learning is available to you right there in your email inbox. This machine learning program learns from all the emails that are present on a server. It should then be able to classify these emails as either spam, not relevant, or ham, relevant to a particular user. This algorithm can course correct itself based on what emails a user reads, what emails a user deletes, or marks as spam. And based on the patterns that it has determined, this algorithm can mark future emails that come in as belonging to the trash or in the inbox. Image recognition is another classic machine learning example. All images are made up of pixels, tiny dots which are either in color or in grayscale. Machine learning algorithms are able to identify edges, colors, and shapes and bring them all together to find patterns. The end result is an ability to identify what that image shows. This is what helps you tag photos in Facebook. Machine learning problems can be divided into four broad categories. Classification involves assigning labels to elements or events. Is the incoming email spam or ham? Was this financial transaction genuine or a fraud? Regression is used to fit a line or a curve on existing data and use that line or curve to predict the values of new data that comes in. Clustering involves dividing data into groups based on how similar they are. Elements within a group are similar to each other, and elements in different groups are different from each other. And finally, rule extraction involves finding if-then patterns that exist within the data. If I regularly buy books on an e-commerce site, is it likely that I'll also place an order for reading glasses? Let's see how machine learning works on a simple classification example. Here, we want to classify an animal. Let's take, for example, the whale as one of two kinds. Is it a mammal, which are members of the infraorder Cetacea, or a fish? It looks like a fish, swims like a fish, and it moves with other fish in the sea. By far, the simplest-sounding way to classify a whale is to have a whole bunch of rules and see which of these rules a whale satisfies. This is a rule-based classifier. A rule-based classifier requires a bunch of human experts who have made up the rules that apply to the classification of animals. The input to this classifier is the whale. The human experts have made up rules, such as mammals give birth to young. They then feed their young for a certain period of time. The whale matches all these criteria, and finally, it's classified as a mammal at its output. In a classifier that depends on machine learning, though, you don't have a bunch of human experts making up rules. Instead, an ML-based classifier will look up an entire corpus of pre-existing data that has been labeled correctly, and try to extract meaningful information from this corpus. Using this corpus, the classifier has kind of figured out what characteristics mammals should have and what characteristics fish should have. It'll then try and apply this to the input. When you use a machine learning base classifier, the information that you specify about the whale at its input becomes critical. Now, if you give it information like, a whale breathes like a mammal, it gives birth and feeds its young like a mammal, then this ML-based classifier will say, that's right, a whale is a mammal. The ML-based classifier relies on the corpus of data that it has already looked at, but it's also very sensitive and highly dependent on what features of the whale you've specified at its input. Now let's say you were to turn around and specify completely different features of a whale at the input. You say that it moves like a fish and it looks like a fish, which is true. In such a case, you'll mislead the classifier into focusing on the wrong things. It looks at its corpus of data and conclude that the whale is a fish, which is clearly wrong. So how did we get this machine learning classifier that we just used? You feed an entire corpus of data of all the features of animals that you have available into a classification algorithm. A classification algorithm has many standard types which we won't go into in this class, such as naive Bayes, support vector machines, decision trees, and so on. This classification algorithm will train itself on this corpus of data and output a machine learning-based classifier, which we use to figure out what categories new animals that we see fall into. Let's quickly look at the differences between an ML-based and a rule-based classifier. The ML-based classifier is constantly evolving. It's dynamic, because as it's exposed to new data, it incorporates new patterns that it can identify. Rule-based classifiers, on the other hand, are static. Once the rules are created, they do not change based on new information. ML-based classifiers do not require experts to draw up rules it should follow. Experts are optional. Rule-based classifiers require human experts to tell the classifier what it should pay attention to. ML-based classifiers cannot function without a corpus of data which they use as training to find patterns that it should look at when it performs its actual classification. Having a corpus of data is optional in a rule-based classifier. And finally, in an ML-based classifier, there is an additional training step where the classification algorithm looks at the corpus of data in order to output a classifier. The rule-based classifier has no additional training step on a corpus of data. We spoke just a little bit ago about traditional ML-based classifiers which rely on what you input to the classifier to make the right prediction. Here, the input is called the feature vector. What you specify within the feature vector is extremely important for the classifier to make the right decision. The output of a classifier is a label. The label is the category that element falls into. Here it is, mammal. Let's say you change this input feature vector to pass in those characteristics of a whale which make it seem like a fish. It moves like a fish, and looks like a fish. The ML-based classifier will label this as a fish. Once again, the input is a feature vector, and the output is a label. A feature vector comprises of all the attributes of the element that you want to classify that you want the machine learning algorithm to focus on when it runs. These are the features. Typically, every data point in a feature vector is a list or a vector of features, which is why we get the name feature vector. The input to an ML algorithm is this feature vector, which the ML algorithm uses to make its predictions. This feature vector forms the crux of the difference between traditional ML algorithms and the modern ML algorithms that we'll look at next. Traditional ML-based systems still rely on experts to decide what features they should pay attention to. The feature vector is the input which you, as an expert, determine. There is a new class of machine learning systems gaining widespread use today. The representation ML-based systems do not need human experts to specify the feature vectors. They figure out by themselves what features to pay attention to.

Representation ML Algorithms

Let's apply first principles once again to understand what is meant by deep learning. In the last clip, we spoke about representation ML-based systems, where you don't need human experts to specify what features of an input the machine learning algorithm should pay attention to. Representation systems figure out by themselves what features are important, what features are typical, and what patterns to find within those features. The traditional ML-based binary classifier receives a corpus of data as its input that runs through a classification algorithm and produces a classifier that we use to make our predictions. An important point that we focused upon earlier here is that the feature selections of whatever we pass in as an input to the classifier is done by experts. You as a user of this machine learning algorithm specify what features are important and what features the classifier should focus on. In the representation ML-based binary classifier, the human experts are once again replaced by an algorithm. We use a feature selection algorithm to determine what the feature vector is that serves as an input to our classifier. You are the human expert who's using this ML algorithm instead of saying focus on a whale where it breathes like a mammal and gives birth like a mammal to get the right label of mammal, you'll tell the representation system nothing. You'll simply feed into it generic information such as an image or a video of whale. It will look through its corpus of data, figure out the patterns within it, and classify it as a mammal. Of course, how right this classification will be depends on a whole bunch of other things. Have you filled in the corpus of data correctly? Are you using the right classifier? How accurate it is, and so on. Just remember that the key in a representation ML-based binary ML classifier is the existence of this feature selection algorithm that takes away the need for human experts to specify the input feature vectors. So this deep learning that everyone keeps harping on about nowadays is simply one type of representation system. It learns the features that are important in order to make its predictions.

Deep Learning and Neural Networks

When I first studied deep learning, I found all explanations that I saw very difficult and kind of hard to follow. What you'll see here in this clip is my effort to really break things down and explain it to you in the simplest of terms. We already know what a representation machine learning system is. Deep learning is one example of that. It's a whole class of algorithms which focus on determining what are the features that are important for this particular problem set? The problem set where the deep learning is applied can be anything, however, deep learning tends to be widely used for speech and image recognition. Another term commonly used along with deep learning is neural networks. Neural networks refer to a class of deep learning algorithms, a class of algorithms that focus on feature selection. Neural networks are built up of core learning elements called neurons. The neuron is the simplest element which has the ability to learn or understand patterns in the corpus of data that has been made available to it. In this very beginners' course, I won't go into the details of how a neuron looks and how different neurons come together to form a neural network. Let's use the same example that we did before, that of a binary classifier which labels the input as one category or the other based on its features. This dotted box that you see here is the feature selection and the classification algorithm that we've set up based on deep learning techniques. The input to this is a corpus of a whole range of images. We'll focus on image recognition for this example and the output here is an ML-based classifier. Deep learning systems are made up of layers, where each layer focuses on recognizing or finding one kind of pattern in the input data. In an image-based deep learning system, you can imagine that the very first layer is focused on individual pixels and extracting information from those pixels. The output are patterns which have been obtained from this pixel layer can be passed on to another layer which focuses on finding edges in the input image. The borders and outlines of the various objects present in the image are identified and extracted in this layer. For a fairly complex image recognition system, you might pass on this output to a layer which focuses on corners of objects. And finally, all of the patterns that have been extracted from these individual layers come together as object parts. You'll have a final layer which puts these various pieces of the puzzles together to identify actual objects represented by that image, a bird or an animal or a house. Within this deep learning algorithm, the layers that we directly feed data into, such as the pixels layer, and the layer that we receive data from, such as the object parts layer in this diagram, are called the visible layers. Basically, the layers that we interact with, either by providing it input or receiving its output, are visible. All the other layers that lie within these input and output layers which form part of the internal processing of this deep learning algorithm are hidden layers. Its input and output are hidden to us. Here, we've seen a specific example for images. You can generalize this for any neural network, solving any kind of machine learning problem. You can have N number of layers within that neural network. If you were to look deep inside a neural network, you'll find that it's made up of a whole bunch of neurons which are interconnected to each other. These neurons pass on information of various kinds in order to achieve their pattern recognition. The first of these layers in an image recognition problem works on pixels. The remaining layers work on processed groups of pixels. Neural networks are so designed that each neuron is a very simple learning unit or entity. These neurons are interconnected in arbitrarily complex ways to recognize patterns. What we just saw was an overview at 100, 000 feet of how neural networks work. The basic structure of how neural networks are set up for deep learning problems, they help find unknown patterns in massive data sets that we can't fathom.

Introducing TensorFlow

That's it for the machine learning and deep learning introductions. Let's focus on the topic for this class, TensorFlow for machine learning. TensorFlow is an open source software library. A TensorFlow implementation has been made available by engineers at Google under the Apache license for free. Engineers on the Google Brain project have been using TensorFlow for some years now. In November 2015, they made an implementation available to all of us. This is a software library for numerical computation. The TensorFlow definition on its website is very general. It doesn't limit itself to machine learning, but that's what it's largely used for. As far as TensorFlow is concerned, all problems in the real world can be modeled as graphs. These graphs are called computation graphs or data flow graphs, and they transform the data as it flows through it, thus enabling very complex numeric computations. TensorFlow has two major advantages as a machine learning library. First up, it has been built for a distributed world from the very beginning. TensorFlow can run on a cluster of machines or on multiple CPUs or GPUs on the same machine. Modeling problems as a graph also lends itself to massively patterning operations. The TensorFlow programming language is actually a part of a suite of software that is made available. It comprises of the programming language itself and its APIs, that is TensorFlow. TensorBoard is a visualization tool which runs on the browser. It allows you to view your graphical model onscreen and see how data flows and is transformed by it. And finally, TensorFlow Serving allows you to productionize the models that you've built very quickly and easily. Let's now see where TensorFlow is usually used, what is good about TensorFlow, its strengths, and what challenges still lie ahead, issues that TensorFlow needs to address in its future versions. TensorFlow is great for research and development of new machine learning algorithms. It has a REPL environment, or an interactive environment, which allows you to prototype very quickly. And you can take the same models that you prototype through to production using the same TensorFlow libraries. TensorFlow does not distinguish between development and production environments, which means that the models that you develop can be moved to production very easily. TensorFlow is great when working with large-scale distributed models. Real-world problems represented as a computation graph lends itself to pattern processing. Different portions of the graph can be computed on different machines in a cluster, thus allowing us to deal with huge data sets. TensorFlow allows you to build models for mobile and embedded systems as well, which means it can run on little memory and a constrained environment. What's good about TensorFlow? TensorFlow has a very easy to use, stable Python API. TensorFlow is built in C++ and also has a C++ API, however, the Python API is the one that's most commonly used and the one that's the best-developed. TensorFlow runs efficiently and gives great performance on mobiles and other embedded systems, as well as less large-scale distributed clusters. TensorFlow was originally open sourced by Google, though now it has a whole variety of contributors from all over the world. Google support ensures, though, that TensorFlow is constantly updated and maintained, bugs within it are fixed, and new interesting libraries added. The object of TensorFlow is to allow you to build machine learning models easily. It doesn't come with a whole bunch of libraries which have these models pre-built for you. In addition, it has tools like TensorBoard and TensorFlow Serving, which allow you to visualize your model and take it to production. TensorFlow also has a few challenges that you have to be aware of. Distributed support exists, but it's not great. It's getting there. Libraries are still being developed, and you'll find as versions change you'll have to migrate your code to follow the newer versions. Writing custom code as you get deeper into TensorFlow is not really straightforward. However, in spite of all of this, TensorFlow is on its way to becoming the default language which you use for machine learning.

The World as a Graph

The world of TensorFlow requires a paradigm shift in how you think about programming. In TensorFlow, everything is a graph. Whatever structures you set up in Python form nodes or edges within a graph. Programming in TensorFlow involves building a graphical model of your solution, which is the network. Every node within this graph is a computation. These computations act on the data that flow into that node and produce the result which flow out of that node. These computations can also be called operators, and that's how they're usually referred to within TensorFlow. The edges which connect these nodes are the data that flows through the graph. The data is transformed by these nodes to produce new edges, which then move onto other nodes. These edges are called tensors. And with this, you get an idea of why TensorFlow got its name. Tensors flow through the graph and get transformed along the way. Don't be intimidated by the word tensor. Tensors are just N dimensional matrices which can be used to represent arbitrarily complex data sets. Let's move from the abstract to the real and associate a computation with every node in this graph. Notice that we are rounding our inputs, calling the floor operation, adding them, multiplying them, finding the absolute value, and so on. Let's assign some simple values to the tensors which form the edges of this graph. The value 1. 8 flows through the node which computes the round of this number and produces an output two. Notice that two can now flow on to multiple computation nodes. The value 3. 6 flows through the floor computation and produces an output three, which is the floor of 3. 6. The node for add now has both its inputs available, two and three. We'll add them and produce the output of five which will then flow on to two new nodes. The node for multiplication can now be computed because it has both its inputs available. You multiply two by five, and you get the value 10. The node to calculate the absolute value of five can also be run, and you get the value of five, which finally feeds into the very last node which has both of its inputs available at this point in time. N plus five is equal to 15. Note that the computation for any node cannot be run till all its inputs are available to it. Every node transforms its input in some predefined way based on the computation that is associated with that node, and some nodes are independent of others and can be run in parallel with them. But we didn't really do that here. We'll see that later on in this course. This is TensorFlow, and every problem in TensorFlow can be modeled like this.

Downloading and Installing TensorFlow

Enough about going on about TensorFlow in theory. Let's get it installed on your local machine and get started working with it. We'll run our very first hello world program in TensorFlow as well. In order to use the Python API in TensorFlow, we need to have some version of Python installed on our local machine. TensorFlow is compatible with a number of Python versions. I have version 2. 7. 11 installed on my machine. You can use any 3. x version as well. We'll use Pip, Python's package management system, in order to install the TensorFlow libraries. Ensure that you have the latest version of Pip installed on your machine. In my case, it is version 9. 0. 1. Your Pip version needs to be above 8. 1 so you don't have problems installing the TensorFlow libraries. We'll download and install the TensorFlow binaries from the TensorFlow website. The Getting Started guide gives you everything you need to get up and running with TensorFlow. Note that you need to know how to program in Python, a little bit about arrays, and some machine learning, but it's not required. Machine learning is definitely not required in order to do this course. The Install tab on the top left of your screen gives you instructions on how you can install TensorFlow. Here you have instructions for the variety of operating systems that you might use. Ubuntu, Mac OS X, and Windows. You can also build and install TensorFlow from its source code. That's an advanced use case, though. Click on the operating system that you're working with. For me, it's Mac OS X. This takes me to a page which has some more install options. You can run TensorFlow with CPU support only or with GPU support. You're going to choose a very plain, vanilla installation here. My machine does not have a compatible GPU, which is why I'll go with TensorFlow that just has CPU support. Running TensorFlow with GPU support, that is a graphic processing unit support, has a whole bunch of additional requirements that you can see when you scroll down this page. We can ignore that for this course. As you scroll further down, you'll see there are additional options and more choices that we need to make. There are four ways to install TensorFlow on your local machine. Of these, we'll choose the virtualenv. The virtualenv is a virtual Python environment which is completely isolated from other Python programs that are running on your machine, so you're running TensorFlow kind of in a sandbox. The virtualenv installation requires you to work on a shell command line, and we'll switch over to the shell to run the commands to download and install TensorFlow. In case you do not have Pip installed, simply run the command onscreen, sudo easy install pip, type in your administrative password, and this should go ahead and download and install Pip. I already have Pip on my machine, which is why I get this message. If you need to upgrade your Pip version, simply run this command instead, sudo pip install dash dash upgrade pip. If you get a message that says that the current user does not own a certain directory, use the dash H command for your upgrade, as you can see onscreen. Once you have Pip installed, go ahead and install the virtual environment, the sandbox where you want your TensorFlow programs to run. I already have virtual environment installed, which is why I see the message requirement already up to date. Notice I get the warning which says that some directory is not owned by the current user. In such a case, run the same command with the dash H option. I'm going to set up a directory TF which I'll use as my working directory. All my TensorFlow code and whatever resources I need will all be set up in here. Create a virtual environment within this directory by running this command, and then activate this virtual environment. Many of these commands are slightly different for Python versions 2. 7 and 3. n. Please make sure you're looking at the TensorFlow website to run the right command. Notice on this page that activating the virtual environment has a different command in Pythons version three and above. In order to activate the virtual environment, you have to run the source command. If you have a bash command line, you'll run source bin forward slash activate. If you have a CSH command line, you'll run source bin forward slash activate dot CSH. Notice that the prompt has changed. You can deactivate your virtual environment by simply running deactivate on your command line. Now we are ready to download and install the TensorFlow libraries. The command for this is once again different based on your Python version and whether you're running TensorFlow on a CPU or a GPU. Make sure you pick the correct command. For Python version 2. 7, I simply run pip install dash dash upgrade tensorflow. I already have TensorFlow installed on my machine, which is why my messages onscreen here might be a little different from those that you might see on your screen. If your version of Pip is less than 8. 1, you need to follow a different set of commands in order to get TensorFlow installed. Go back to the install documentation here and follow these commands. You'll have to click through to that link to find the correct TF binary URL that you need to use for your Pip install command for TensorFlow. Here you see for Python 2. 7, CPU and GPU support, these are the URLs and similarly you'll find commands for Python version three and above as well. At this point, you've successfully downloaded and installed TensorFlow on your local machine using Pip. Now, let's get into our Python interactive command line shell, that is, the Python REPL, REPL stands for read-evaluate-print-loop. The REPL is basically an interactive command line which allows you to type in your Python commands and get the results right away. This is our TensorFlow hello world program, and the very first step here is to import your TensorFlow libraries. The common practice is to refer to this library as TF, this is a handy short form so it doesn't get tedious to reference these libraries over and over again. The next step will instantiate a constant called hello which holds the string hello, tensorflow world. Tf dot constant is what we use to instantiate this. In order to execute any bit of code that you've set up in Python on TensorFlow, you need a session. This you instantiate by calling tf dot Session. We'll learn more about sessions in the modules to come. And this bit of code ran successfully, but a whole number of warnings were generated. We can ignore these warnings for now. They basically indicate that if you build TensorFlow from scratch using the source code, it can run much faster on your machine. That is if you specify the options correctly. The pre-built binaries that we use may not be the most effective to run on your machine. And finally, to evaluate that one line of code which instantiated a TensorFlow constant, we call session dot run and we print it out to screen. And there you see it, our TensorFlow hello world. And with this, we come to the very end of this module. This module started off with the basics of machine learning. We learned how traditional ML-based systems differed from representation ML-based systems, and also understood what deep learning and neural networks are. This module introduced the TensorFlow library for numerical computation. We understood the strengths and challenges of using TensorFlow for machine learning. We got a brief insight into the world of TensorFlow. Every model, everything in the real world, as a computational graph. And finally, we got TensorFlow up and running on our local machine. In our next module, we'll understand the computation graph, the basic model used to represent all TensorFlow programs. With that, we'll also practice simple TensorFlow operations in order to wrap our head around basic building blocks of TensorFlow.

Introducing Computation Graphs

The Computation Graph

Hi, and welcome to this module on the computation graph. The computation graph is the way TensorFlow represents any model in the real world. In this module, we'll start off by understanding how we model nodes, edges and dependencies in a computation graph. We'll write some very simple code in TensorFlow and use that to understand the basic layout of a TensorFlow program. We'll run these programs and we'll also see how write out information or events onto log files such that they can be parsed and visualized with TensorBoard. In the last module, we spoke of the computation graph in TensorFlow. In the world of TensorFlow, everything can be modeled as a graph. The nodes in a graph are the computations or the operators that act upon data. And the edges in a graph is basically the tensors of the data that are transformed by flowing through these nodes. Data within the computation graph are called tensors. These are nothing but end dimensional matrices used to represent massive data sets. We also saw a simple example of math computations, which can be modeled as a TensorFlow graph. If you assume that the input to this graph is represented by A and B, the equation that is represented by this graph can be written by as you can see on screen. Y is the final result that flows out of the other end of the graph. Let's understand some of the basic characteristics of a TensorFlow graph. Notice that this graph is a directed-acyclic graph. Every edge in this graph has an arrow, that is, it points towards a certain node. This is the direction in which data flows. These edges point forward towards a result. This is a directed graph. The computational nodes within this graph depend on other nodes to forward a data before it can perform its computation. One node can send its output to multiple nodes. Or it can receive inputs from multiple nodes. In such a case, the node can perform its computation only after all its inputs are available. The summation or the plus node, that you see here on screen, depends on the computation round and floor to be completed before the plus operation can be performed. This is how we model dependencies in the real world. Nodes which received data from other nodes are dependent on those nodes. This can be a direct dependency, where there is one edge connecting one node to another or the dependency can be indirect. The summation node at the very end depends on the input node. But there are several other computations that have to be performed on the data along the way. If you look at the structure of this graph carefully, you'll notice another important characteristic. There are no cycles in the graph. If you look at all the edges, they're directed forward in one direction. This graph is acyclic. Let's go ahead and change this graph a little bit and create an artificial cycle. We'll turn around one of the edges to point in the reverse direction. Because of the change in the direction of this arrow, there is a cycle right there between the round summation and the multiplication operator. A real-world problem with a cycle in it has a fundamental issue. A graph that cycles will never finish computation. You'll be stuck in this loop where the output of the nodes are sent back as its input and the computation will never end. Problems, which you solve using TensorFlow will be represented as a directed-acyclic graph.

Modeling Cyclic Dependencies

Well, we confidently stated in the last clip the TensorFlow models real world programs as directed-acyclic graph. But the fact remains that machine learning in particular tends to have feedback loops, which are basically cycles introduced in any graph. So how do we square the circle? We just said that a graph that cycles will never finish computation. However, the process of machine learning involves a feedback, which is in effect, a cycle. TensorFlow builds with iterative feedback loops without introducing cycles in the computation graph. Let's see how. Let's say that we have some machine learning algrothim which does feature selection as well as applies an algorithm to say classify your input data. So we have the corpus of data at the input and a label at the output. Once we predict an output using any machine learning algorithm. And this output can be of any type. It can be classification, regression, you name it. The algorithm with calculate in some way how accurate its prediction was. If the algorithm finds that it is very lossy or inaccurate, it will then provide a feedback, loop back into itself, tweak some of the variables used within the algorithm, and recalculate its ouput. Within this machine learning algorithm, you can imagine a very complicated neural network. And this feedback loop serves as another input to this neural network so that the original parameters can be tweaked and a process can be run once again. Machine learning is all about feedback loops. And these feedback loops introduce a cyclic dependency in our graphical model. That means tensor flow has to have some way in which it can model these cyclical dependencies. In this graph here on screen, the last node feeds back into the very first node at the botom. we model cyclical dependencies by unrolling the graph. The output of the last node is fed into a copy of this graph, which we make. Instead of feeding the output into the first node of the same graph, you feed it into the first node of this copy. We can repeat this process as many times as we want. Unroll this graph, make as many copies as required. These form the multiple iterations of this algorithm. Machine learning is essentially a bunch of iterative algorithms which get closer and closer to more intelligent decisions. How much you unroll your TensorFlow graph depends on the number of iterations you want to run. Even though TensorFlow models the world as a directed-acyclic graph, when you run multiple iterations, it'll automatically unroll graphs to model cyclic dependencies, which are very common in machine learning.

Building, Running, and Visualizing Graphs

Before we get much further into the theory behind the computation graph, let's understand the basic steps in a TensorFlow program and see it applied in an actual demo. Every TensorFlow program will first involve building a graph. You specificity the various operations that you want to perform on the data that you have. And the next step is to actually execute the graph to get the result that you're interested in. Let's build and run a graph in TensorFlow. We'll also explore the graph using our TensorBoard visualization tool. We start off by building the graph. We enter the Python interactive shell by running Python on our command line. Notice that we have within our virtual environment. The first step here is to import the TensorFlow libraries as TF. We'll then set up four constants, A, B, C and D, which we'll use as the input to our computation graph. We'll perform a whole bunch of computations on these constants. Constants in TensorFlow are immutable. So once you assigned a particular value to it, you cannot change it within the course of the program. TF. constant is what you use to specify constants. Notice that in additional to the value of these constants, there's just six, three, 10 and five, I've also specified an additional name parameter. This name parameter allows us to identify these constants when we visualize it using TensorBoard. So they are pretty important debugging tool. A, B, C and D are the tensors that we float through this graph. I haven't formally defined tensors, yet. Don't worry, we'll get to that in just a bit. A, B, C and D are edges in our graph. Mul is a node which specifies a multiply operation. It works on A and B, which means it multiplies A by B. Computational nodes can also be identified by names. In this case, it's called mul. Let's add in our division operation, tf. div, which divides C by D. And the name is div. The tf. add_n operation sums up the elements in an array. Here, we've specified two computational nodes within the array, which means the output of our multiplication and division is what needs to be added. So we want mul plus div. All right, we have all these nice computations, let's print out these results. We print addn and we get something strange in return. We know that it's a tensor, okay, and it has addn:0, which is the name of the tensor. It has a shape and a data type, which is an int32. But, really, that's not what we expected. We, actually want the value of addn after it has performed all the computations. What about the value of the constants that we fell in? Let's print out A and we see that it's a tensor. It's called constant_a:0, it has a shape and a data type of int32. The same with the mul computation node and anything else that we print out. Printing data using the print statement just gives us the name of the tensor, the shape and its data type. It doesn't give us the value that it holds at any point in time. This is because we haven't run the graph that we built up. All these operations constructed a graph in TensorFlow. But we haven't executed it to get its values at runtime. And this we'll do in the next step in the TensorFlow program. We'll execute the graph to get results. This might seem highly non-intuitive to those who are used to programming languages, but this is how TensorFlow works and it's for a very good reason. TensorFlow involves constructing huge computation graphs. The entire graph has to be construction first before it's executed. This allows TensorFlow to see efficiencies and parallelization mechanisms it can use on the graph and then allows it to split it up across multiple machines or CPUs before it executes it. Well, in order to run a graph, you need a TensorFlow session, a session object which you can instantiate by calling ef. session. It's responsible for supervising the execution of any TensorFlow graph. It doesn't mater how large or small your graph setup is, it requires a session to execute it. In order to execute one portion of a graph, you simply call session. run and pass in the node that you want computed. In this case, it's the final add_n node. Add_n basically calculated, A multiplied by B, plus C divided by D, which gave us the answer 20. You can call the session. run on any of the intermediate nodes as well. Session. run on the multiplication node will give you the answer 18. And session. run on the div node will give you the answer two. That is, 10 divided by five. Calling session. run on one particular node of the graph performs all the computations 'til the output of that node is needed. It doesn't compute the portions of the graph that are not in the part for that node. Now that we've built and executed a graph. Let's visualize it using the TensorBoard browser to it. This requires some additional instrumentation that you need to add to your code. You need to instantiate a file writer, which is in the tf. summary namespace, which allows you to write out events to a log file directory. The very first argument that we pass in to this file writer is the directory where we want the events to be stored. This directory is m2-example1, the first example in module two, and what we want to write out is the graph representation. This writer can be used to write out a whole number of summary statistics to your log files, which can then be visualized on TensorBoard. The writer, object and even the session are handles to resources. And these should be closed at the end of every program. It's just good practice. You don't want to leave them open hanging. Exit from the Python environment, and run an LS command and you'll see that the the m2_example1 directory has been created in your folder. In order to get TensorBoard up and running, simply run the TensorBoard command on your command line. And specify the log directory where the events have been stored. Here, it is is m2_example1. The messages on screen will show you which order TensorBoard runs on. Here it is 0. 0. 0. 0:6006. If you open up this port on the browser, you will find TensorBoard. TensorBoard has a whole number of tabs on top, which show you the different types of summary statistics that you can write out to your log files. Here, we'll focus on the graph that we've written out. Go to the Graphs tab and there you see it, our computation graph in all its glory. Notice that constants A and B are faired into the mul node. Constants C and D are faired to the div node. And the output of the mul and the div goes into the addn. Each of these nodes and constants have the name that you specified within your Python program. You can click on these individual computation nodes in order to get more information about it. For example, when you click on addn, it it'll show you its attributes and its inputs. Mul and div are inputs to addn. Similarly, you can click on the mul operator. See that it operates on constants A and B and feeds its output to the addn node. The same thing with the div operator. TensorBoard is an extremely handy visualization and debugging tool, which you should explore.

Computation Graphs and Distributed Systems

The demo in the last clip would have given you some idea of how to build up and run these computation graphs. The graph that we looked at though was a teeny tiny really simple one with few nodes and few tensors. Real world computation graphs look like this. These graphs are typically not built by hand, but by using TensorFlow libraries, which generate thousands of nodes to perform computations. TensorFlow gives you the ability to compute the value of one node at a time. Let's say this maroon node is what you wanted to compute, TensorFlow will simply run those computations and act on. those tensors flowing up to that node. The remaining computation nodes and tensors will remain un-executed. Or if you choose to run another node, then all the tensors and computation nodes that feed into this blue node will be executed. TensorFlow essentially finds the most efficient way to compute a node. This optimization is an important reason that TensorFlow computations are performed in two steps, one where you build up a graph and another where you execute it. Once it has a picture of how the entire graph looks like, it'll only calculate those parts of the graph which are required to find the result you're interested in. TensorFlow's view of the world as a computation graph also enables parallelizing operation. This graph can be easily split up into multiple portions. And each of these portions can run on a different node in a distributed system. Some of these can run in parallel and some of these have dependencies. But TensorFlow can handle all of that with ease. This graph representation of models in the real world has this additional advantage that multiple portions of the graph can be run in parallel across machines in the cluster.

Simple Math Operations

In this demo, we'll brush up on our TensorFlow and TensorBoard skills by writing simple math commands in TensorFlow. This time, we'll use a Python editor rather than the Python Repl environment. This is typically where you'll write your larger TensorFlow programs anyway. We start off by important TensorFlow as TF. And we declare a bunch of constants. Notice A is a constant, but it now has a float data type. It's not an integer. So we have B, C and D, all floating point constants. Each of these constants have a name so we can identify them in our TensorBoard graph. TensorFlow offers a whole range of math operations that you can perform on scalar data as well as on matrices. Here, we see the square operation, which calculates A square, which is the tf. squre. You can see the entire Python map API at this URL that you see on screen right here. Let's add in some mode computations. There you have the power, tf. pow, which calculates B raised to the power C and the square root operation, which calculates the square root of B. Square, power and sqrt are all computation nodes in our TensorFlow graph. We'll feed the output of all these computation nodes into a final sum, which sums up the result of square, power and sqrt. This set of Python commands that we see on screen have instructed our computation graph. In order to run it, we'll instantiate a session. Remember, a session is the instant that coordinates the execution of our TensorFlow graph. Let's execute various portions of this graph. Session. run, by passing in square, will execute all the nodes up 'til square. Call session. run on each of these nodes so we get all the intermediate results as well as the final result. That is, the final_sum. Remember that as a good citizen, you should always close your session when you've completed your program. I've forgotten to do so here. A session is automatically closed when your Python program is terminated. But you shouldn't rely on that. You should only keep the session open for as long as you need it. Run this Python program just like you would any other Python program. Use the keyword Python and specify your. py file. Here, it's called m2_simplematch. py. And here are the results of our math operations. We found the square of A, power of B, raised to C, the square root of D, and the sum of all these intermediate results. Nothing really fancy here. This was a really simple Python program, but it's for you to get practice using the Python constructs. The only thing that remains here is to write out our graph to the log events file so that we can visualize it in TensorBoard. We'll do it using the summary. filewriter. Close the writer as well as the session at the end of the program. Since we've added new code to populate the log events file for TensorBoard, we rerun the program, we get the same results as before. But, this time, if you run an ls-l operation here, you'll notice that we have a new directory that has been created. That is, the m2_example 2. This is where the events live, which will we point TensorBoard to. Explore this directory and you'll see the log event files that TensorBoard uses. Each time you run this program, new events or event files will be added here. Let's see what our TensorBoard visualization looks like for our simple math program. Point the logdir to m2_example2. Go to the same port, 6006 at IP address 0. 0. 0. 0. And under the Graphs tab here, you'll see the computation graph for the simple math program that we just set up. The input to this graph is all constants. Constant A, B, C and D. As usual, you can click on the individual computation nodes to see the input that goes into that node and the output of that node and where that output is fed to. Here, the square of A takes in a constant and feeds it output to final_sum. When you click on final_sum, that is our final computation node, you can see the inputs to that are the three computation nodes for square, power and square root. You can use the box on the right to trace back and see where the data for final_sum comes from. Leave TensorBoard running and let's go ahead and make modifications to this Python program. We'll add another computation node called another_sum, which adds the constants A, B, C, D, and the output of the power computation node. Notice the TensorBoard is still running. Create a new terminal window tab and rerun this Python program. Activate the TensorFlow virtual environment before you do this. Once the program has run through to completion, switch over to the terminal window where you have TensorBoard running. And notice that there is an additional message there. TensorFlow gives a warning saying: Found more than one graph event per run. Overwriting the graph with the newest event. TensorBoard has automatically picked up the new computation graph that we've added to our Python program. Switch to your browser, refresh the window and you'll see under the graphs tab that the new tab, another underscore sum, has been added. When you're working on a program, you can simply leave TensorBoard running. It'll automatically pick up any changes that you make within the program and any new summary or graph information that you write out using the file writer.

Tensors

In all the clips that you've seen so far, we've spoken a lot obout tensors being used to represent data. We haven't formally defined that term yet. In this clip, we'll do so. In the two simple demos that we've seen so far, we've only used constants. These constants are also tensors. You can find the definition of a tensor at the official TensorFlow website. It is the central unit of data in TensorFlow. All data in TensorFlow is represented as a tensor. A tensor is made up of primitive values such as integers, floats, strings and Booleans, which is in an array form, can be shaped into an array of any number of dimensions, which is why we say a tensor is an n dimensional array. So far, all the constants that we've instantiated have bene scalar data. Scalars intensive flow are zero dimensional tensors, A single unit of value such as 3, 6. 7 or the string A are all scalars. A single dimensional array or a list, also called a vector, are one-dimensional tensors. The array of integers that you see on screen here is a 1D tensor. The number of dimensions that a tensor has can be determined by the number of square brackets that we use to represent that tensor. Here, we have a single pair of square brackets. Its a one-dimensional tensor, also called a list or a vector. There is no limit on the number of dimensions that a tensor can have. A 2D matrix is a 2D tensor. Notice that we have two pairs of square brackets here. The two dimensions of a matrix are its rows and columns. And in order to access a single element in a matrix, you need to specify two indices, one for the row number and one for the column number. This is a 2D tensor. If you imagine multpile matrices stacked one on top of another, what you get is a three dimensional tensor. This can be extended up to N dimensions, and they are called N-D tensors. The example that you see here on screen is a 3D tensor, and it requires three indices to access a single element within this tensor. Every tensor can be characterized by three specific properties. The first of these is the rank of the tensor. The number of dimensions present in a tensor its call is rank. The second is the shape of the tensor, the number of elements in each dimension of a tensor is its shape. And finally the data type that it holds, whether it's an integer, floating point, string, et cetera. Let's see some examples to understand the rank of a tensor. So we have a tensor and its corresponding rank. Scalars have no rank. The rank is zero. They're called 0-D tensors. A vector has a rank of one, there's just one dimension. A matrix has a rank of two, there are two dimensions. And a 3-D tensor, specified by three square brackets has a rank of three. And N-D tensor, you can extend this logic, it'll have a rank of N. Let's now understand the concept of shape. This specifies the number of elements that the tensor holds in each dimension. A scalar has a shape of simply square brackets. A scalar does not have any number of dimensions. A vector, on the other hand, has a single dimension. And this particular vector that you see on screen has three elements. It has a shape of three specified N square brackets. If you change the number elements in a vector, its shape will tape. For a tensor of rank two, which is a two-dimensional tensor, the shape specifies the number of elements in its rows and its columns. The example that you see here on screen is a tensor of shape 3, 2. Three elements in its rows an two elements in each of its columns. The 3-D tensor that you see on screen here has a shape of 2, 2, 1. Two layers, two elements in each of the rows of the layer, and one element in each column. And the last defining characteristic of a tensor is the data type that it holds. It can be an integer, int8, int32, a floating point, a string or a Boolean. The rank, shape and the primitive data type that it holds are the three important characteristics which define a tensor.

Rank of a Tensor

While debugging your computation graph, you'll find that you'll need to know the rank of the tensor at certain times. You can use the tf. rank method in order to find your tensor rank. Let's move into the Python Repl environment, import the TensorFlow libraries, instantiate a session, and let's set up some tensors of various ranks. Here is a zero dimensional tensor with a rank of zero. It's simply a scalar. Call tf. rank on this 0-D scalar. Remember you have to pass it to a session. run if you want TensorFlow to execute it. It has a rank of zero. Let's set up a one-dimensional tensor. Here, 1-D is a constant, which has the strings how, are and you, specified within square brackets. The rank of this constant is one. Similarly, you can instantiate a constant which is a two-dimensional matrix. And if you run tf. rank on this, it'll give you a rank of two, and so on extending up to the N-dimensional matrices, with a rank of N. Here is a three dimensional one. Make sure you close the session before you exit out of the repl environment.

Tensor Math

In this demo, we'll once again perform simple math operations. But this time, we'll get it to operate on tensors, that is multi-dimensional tensors rather than scalars. Import TensorFlow as before. Set up a constant, which is a vector that is a one-dimensional tensor. It has the elements, 100, 200 and 300, that is X. Set up another tensor Y, which is, again, a 1-D tensor with values one, two and three within it. TensorFlow supports a whole range of math operations, which act on all elements within a tensor. These are math operations which you'll use far more often than other simple ones that we saw earlier. For example, the reduced sum sums up all the elements within that 1D tensor X. Similarly, the reduce_prod operation here acts on all elements of the tensor Y, and finds its product. Here it is one multiplied by two, multiplied by three. Those are the elements within the tensor Y. The result of the reduce_sum and the reduce-prod operation reduces a tensor having the same dimensions. In fact, the tensor is a scalar, a 0-D tensor. Two tensors of the same dimensions can be used in a division operation or other math operations. You also have reduce-mean, which finds the average values of the elements in a tensor. Here, we'll specify the 1-D tensor on the fly by using the square brackets and passing in the results of sum X and prod Y. Let's execute these by instantiating a session. We want all the intermediate values to be calculated as well. So we want to print out the values of X, Y, Sum X, Prod Y and then finally final_div and final_mean. Let's write out this graph to visual in TensorBoard, because it's the first time you're using 1-D tensors rather than just scalars. Run this Python program on your command line. Notice one thing here, if you look at how my command prompt looks, you'll find that we are not within the virtual environment, which means the TensorFlow program that I'm running now is not isolated from other Python programs. But because it doesn't do anything very critical, this is okay. This explains the difference in the prompt that you see on screen. And the result here, you'll see here, is the sum X that is 600 prod Y, that is one multiplied by two, multiplied by three, that is six, 600 divided by six is 100, and the average of 600 plus six is 303. Run TensorBoard by pointing to the right log directory. Switch over to your browser window and to the graph stabbed within it. This is what the math operations that you just set up look like. Notice the computation nodes final mean and rank are in a different color. This is because we have scoped them in a separate name scope. We'll study name scope a little later in this course. This graph also seems to have some computations which we haven't explicitly specified in our computation graph. For example, rank, rank is needed to perform some of the other operations that we've specified. Behind the scenes, TensorFlow adds in those computation nodes which it requires in order to perform the other operations that we explicitly specified. In our particular example, TensorFlow needed to calculate the rank of some of the matrices that we use. Given that its inputs are sum X and prod Y, it was probably for the mean calculation.

Numpy and TensorFlow

If you've done any kind of numeric computation at Python, you've probably used the numpy package. Numpy is a very common and popular package for scientific computing in Python. And amongst all other calculating libraries that it offers, it contains a very powerful array representation which has become the industry standard in Python. TensorFlow is perfectly compatible with numpy, in that numpy arrays are simply tensors and TensorFlow can treat them as such. In order to install the numpy package, you can simply use pip install numpy. I already have it on my machine, which is why you see this particular message. Let's go into the Python Repl environment and see how numpy erase can be treated as tensors. TensorFlow and numpy are so tightly integrated that TensorFlow data types are based on those from numpy. In fact, if you compare the primitive types of numpy and TensorFlow, Python accepts them as being exactly the same. Let's set up a zero dimensional array in numpy, using np. array. We've imported numpy as NP. You can now treat this numpy array exactly like a tensor. If you call tf. rank on it, you'll get a rank called zero. Tf. shape, which gives you the shape of tensors will also run on this numpy array. Notice the shape is just square brackets. And the array has a datatype of an int32. Let's now instantiate a one-dimensional numpy array or a vector, run the rank and shape methods on this. And TensorFlow works on this exactly as though it were a tensor. Rank of one and a shape of four elements in the one dimension. And with this, we come to the end of this module on the computation graph. We saw how every TensorFlow program is modeled as a directed acyclic graph. Cyclical dependencies or feedback loops can be modeled by unrolling the graph as many times as we need to. We introduce some very basic constructs of TensorFlow such as constants. The operators would work on them and the session which runs a TensorFlow graph. We learned that tensors are nothing but end-dimensional matrices and learned important tensor characteristics such as rank, shape and its data type. And, finally, we introduce a very basic debugging and visualization tool in TensorFlow, the TensorBoard. In the next module, we'll focus on understanding some more fundamental constructs of TensorFlow such as placeholders and variables, and see why they're needed when we work on drill machine learning problems.

Digging Deeper into Fundamentals

A TensorFlow Example: Linear Regression

Hi, and welcome back. In this module, we'll be getting deeper into the fundamentals of TensorFlow, and in order to do this, we'll look at the logic of a machine learning program in the real world. We'll take the example of linear regression, which is very simple and easy to understand. So far, we've been working with constants in TensorFlow. However, we need the ability to run the same model on different problem sets, which is why we'll need placeholders and feed dictionaries. Machine learning problems often involve estimating certain values and then adjusting our initial estimates based on the outcomes that we've calculated. We require variables to hold values which the program constantly updates. As our TensorFlow programs become more complex, our visualization on TensorBoard also needs to keep up. We'll make TensorBoards more useful using named scopes. Regression models are a standard type of machine learning algorithm. Regression tries to model a relationship where there is a cause and an effect. The cause is an independent variable. It occurs by itself. The effect is a dependent variable because it depends on the cause. Modeling this relationship, x causes y, can be done by a regression, and if we estimate that this relationship can be represented by a straight line, it's called a linear regression. An example of a relationship that can be modeled by linear regression is the hypothesis that wealth increases life expectancy. As people become richer, they tend to have better medical care, which is why they tend to live longer. So the cause is the wealth of individuals, and the effect is their increased life expectancy. This can be a straight line. Another relationship that could be modeled using linear regression is the fact that, as we move further away from the city, home prices will tend to be lower, so the cause is distance in miles from the city center, and its effect can be seen on the price per square foot of homes. The cause is also called the explanatory variable, and the effect is the dependent variable. It's helpful to visualize this graphically. Let's model this on a two-dimensional axis. The x-axis is the cause, or the explanatory variable, and the y-axis is the effect, or the dependent variable. Let's say x is the distance from the city center; y will be the price per square foot of a home. Let's say that we have a whole number of data points for prices per square foot of homes and the distance of that home from the city center. What we really want is a simple algorithm that will allow us to predict the price per square foot of a home given the distance of that home from the city center. This relationship can be represented as a straight line, which is why it is a linear regression problem. You can set up a regression line which is of the kind y is equal to A plus Bx, a very standard equation of a line, which you're probably familiar with. And using this line to model the relationship between the price per square foot and the distance from the city center of a home, our model can predict the price per square foot given the distance. Now, let's figure out how exactly we get this line which can be used to represent these points. Every point belongs to some line, which can be represented by the equation y is equal to A plus Bx. If you have a number of points on a two-dimensional plane like you see on screen, there are a number of lines that can be drawn through this plane. Linear regression involves finding the best fit line. Let's define what best fit really means. Let's start off with two lines here. Line one satisfies the equation y is equal to A1 plus B1x, and line two satisfies the equation y is equal to A2 plus B2x. Focusing our attention on line one, we can see that the intercept on the y-axis for this line will be A1. That's the significance of A1 in this line equation. A1 is the y-intercept. B1 represents the slope of the first line. This means if x increases by one unit in the first line, y decreases by B1 unit. This is the definition of slope. y decreases as x increases because this line has a negative slope. Let's now consider line two. If we extend line two so that it intercepts the y-axis, it'll have the y-intercept A2. The slope of line two is B2. As x increases by one, y will decrease by a value of B2. This line also has a negative slope. Our objective here is to find the best fit line, and this we do by minimizing the least square error of the points that we wish to represent with this line. Minimizing the least square error means finding the line which is as close as possible to all our data points. In order to find the least square error, drop vertical lines from all these points which serve as input to our regression. Drop these vertical lines to line one and to line two. In this case, by visual inspection it's possible to see that the best fit line is actually line one, but it has to be formalized mathematically, and minimizing the least square error does exactly that. The best fit line is one where the sum of the squares of the lengths of these dotted lines that you see on screen is minimum. These dotted lines represent errors, errors between the actual points and the regression line that we've fitted to represent those points. Now that we know that these lines represent errors, the best fit line is the one where the sum of the squares of the lengths of the errors is minimized. The line that best fits our input data set improves the predictive power of our regression algorithm. Finding the best fit line is important because it'll give us the best prediction. Given the distance from the city center, the best fit line will give the best prediction for the price per square foot of a home, and that, in a sense, is what machine learning is all about.

Linear Regression in Practice

In the last clip, we saw how using linear regression to find the best fit line through a set of points allows us to model a linear relationship between cause and effect. This is a classic example of a machine learning algorithm. That was the theory behind how linear regression works. Let's see how it's applied in practice. The equation of a line is y is equal to A plus Bx, where A is the y-intercept, and B is the slope of that line. A machine learning algorithm, in order to find the best fit line, will estimate the initial values for A and B. So A and B are given some values to start off with. It'll run the regression, and it'll find the errors for the regression lines with those values of A and B. Once these errors have been calculated for one iteration of linear regression, these are then fed back into the input to get new values for A and B. This feedback loop allows the algorithm to tweak the values of A and B in such a way that we find the best fit line. Let's see this iterative process for finding the best fit line visually. These are the data points which represent the cause-effect relationship that we want to model using a linear model. We start off with some values for A and B which gives us an initial line. Let's draw this line randomly somewhere on our plane. Astart is the y-intercept for this line, and Bstart is its slope. Using the points in our input data set, we calculate the least square error, which is the distance of the points that we want to represent and our current estimate of the line. We feed this least square error back into our regression algorithm in order to find better values for A and B. With these better values, our line will move closer and closer to where the best fit line should be. We get new values for A and B at every iteration. We are constantly minimizing the error that's fed back into this machine learning algorithm 'til a point where the error has reached the minimum. We'd spoken about this iterative feedback of errors earlier in this course where we spoke about how TensorFlow handles the cycles introduced by these feedback loops by unrolling the graph. This is where we have our best fit line, and this best fit line is called the regression line. Regression is a classic example of a supervised machine learning algorithm. In a supervised algorithm, we have a training data set with the correct output values. These correct output values allow the algorithm to adjust itself 'til the errors are minimized. We'll talk about the differences between supervised and unsupervised learning later on in this course.

Placeholders

So far in this course on TensorFlow, we've seen constants, sessions, and some math computations. Let's focus now on placeholders. We'll use regression as a machine learning example to demonstrate why placeholders are needed in TensorFlow. Earlier in this module, we understood more about machine learning by taking an example of linear regression. We can draw a straight line through the points that are presented on this plane in order to predict the price per square foot of homes based on the distance from the city center. Based on the negative slope of the line, we can tell that the price per square foot of home falls as we move further away from the city. Linear regression can be used to model many other use cases. For example, you might want to predict the price of Google based on their Dow Jones index. It's quite likely that the linear regression line that represents this cause-effect relationship slopes upward. Or you might want to model the cause-effect relationship of wealth versus the life expectancy of individuals. As individuals get richer, their life expectancy increases because they have more access to preventive as well as curative care. These are just very few examples for one kind of machine learning algorithm. Machine learning can be applied to a variety of problems. So for the same model, in this case the model was linear regression, we can feed in a whole bunch of input data, which means your model should have the ability to accept different input values. In the particular case of linear regression, these are the different x and y values for our data points, and this is where we need placeholders in TensorFlow. Placeholders hold the place for a Tensor whose value will be available to it only at runtime. You feed in the value you need, which is the input data set that you want your machine learning algorithm to run on. Let's switch over to a demo to understand placeholders. We'll use the same simple math operations that we saw earlier. We have to feed in our input data set to the placeholders, and this will be done via a feed dictionary. This is when we execute our graph using a session. We set up a placeholder in our TensorFlow program by using the tf. placeholder method. This placeholder is of type integer. It has a shape of three, meaning it's a 1-D vector with three elements in it. We name it x. We'll use another placeholder, y, with the same dimension and shape, and name it y. Now, for the rest of the program, we treat the placeholders exactly like we treated the constants that we used before. We perform the reduce_sum on the x placeholder, and we run the reduce_prod on the y placeholder. The only difference here is that we do not know what values x and y will hold 'til we run the graph. Whatever value we get for sum_x and prod_y, we'll divide sum_x by prod_y, that is final_div, and final_mean will run the average operation on sum_x and prod_y. Instantiate the session in order to run this graph. Now, here is where the difference comes in when you use a placeholder. In order to calculate the sum of all elements in the x Tensor, we need to know what the x Tensor actually contains, and this is what we feed in using the feed_dict parameter passed into Session. run. This is the feed dictionary where we specify all the placeholders that are required to run this computation. Only x values are needed here. We only feed in the x Tensor. For computing prod_y, we need the y Tensor. So the feed dictionary for this computation takes in the value of the y placeholder. It does not require the value for x. Let's write out this graph with placeholders out to TensorBoard logs so that we can visualize it. First, we run our simplemath_with_placeholders program. We got the same result as before. The sum of all elements of x is 600, and the product of all elements of y is six. Run tensorboard, and you'll see that the output that you see within the graph section looks pretty much the same as before. The one difference here is that x and y are both placeholders, and if you click on them at the top right, you'll see the details of that placeholder. It's type is INT32. It's shape is a 1-D Tensor with size three. The same is true for placeholder y. It clearly says there that these are placeholders. Let's switch back to our editor and run some more computations. This time we want to calculate the value of final_div. For final_div, though, we need to feed in values for the placeholders x as well as y. Final_div, in order to be computed, requires both these values to be filled in. The same thing is true if you want to calculate the value of final_mean. Notice the feed dictionary, and notice that we've specified values for both placeholders, x as well as y. Switch to the terminal window and run this Python code. The results should show you that every computational node has been computed with the x and y values that we fed in in its corresponding Session. run command.

Fetches and the Feed Dictionary

This demo will focus on the two important parameters that we pass into every Session. run command. The fetches parameter indicates what it is that we want to compute, and the feed dictionary specifies the placeholder values for that computation. In this example, we perform a very simple math computation, y is equal to Wx plus b. Here, W is a constant Tensor. It is a 1-D Tensor with two values in it, 10 and 100. x and b are both placeholders. We'll have to feed in values for these when we actually execute our graph. x and b are both placeholders of type int32. While instantiating these placeholders, we haven't specified a shape for the Tensor that it holds, which means these placeholders can be Tensors of any shape. We first get the value Wx. We multiply W by x. tf. multiply is simply multiplication and not matrix multiplication, which means that every element of W will be multiplied by every element of x. W and x have to be compatible Tensors, which means x has to be the same shape and rank as W. That is, a vector with two elements. In our math computation, we get the final value of y by adding Wx plus b. In all the code that we've written so far, it was fairly tedious to keep track of sessions and to remember to close them when you are done. The usual way to work with sessions in TensorFlow is by using the with command in Python. All the code that requires a session you write within this block. Let's first calculate the result. W multiplied by x we call Session. run, and the first parameter is the fetches parameter. We want to compute Wx. Fetches refers to the node of the graph that we want to compute. The next parameter is the feed dictionary where we pass in the values for our placeholders. In the computation Wx, the only placeholder is x, so this feed dictionary only has values for x. In the next statement, we calculate the final result, Wx plus b. Here, we need to specify the values for two placeholders, both x and b. Notice that the values that we specify for x here are different from the values that we specified for x in our previous statement. Think of every Session. run as an independent calculation of the node that we've specified. You can feed in whatever values you want for that particular computation. It's always good practice to write out your graph to TensorBoard so that you can view it for debugging. We won't be observing the TensorBoard here, though. Let's run this code and see what the result is. The intermediate result for W multiplied by x gives us 30 and 3300. W had the values 10 and 100, and x had the values three and 33 that we fed in. The final result, we fed in different values for x and b, and we got the result 57 and 5009. When you are computing a node in a graph, you do not need to specify every value from scratch. You can also specify intermediate values. In this statement, we used this feed dictionary to specify a value for Wx. Instead of specifying the values for x and b, which are our input placeholders, we can instead specify the precomputed value of W multiplied by x, that is, Wx, in the feed dictionary. Here, the value of Wx is 100, 1000. Allowing us to specify intermediate values is very useful for debugging. If you want to debug one particular computation, specify intermediate values to make things easier for you. If you run this bit of code, you'll find that the result with the intermediate specified is 107 and 1009. You can also calculate the values for multiple nodes using one Session. run statement. Let's calculate another result that is y_, which is x minus b. y_ is just an example of another result that we want to calculate. We want to calculate the values of the nodes y as well as y_ in the same Session. run statement. The fetches parameter should simply be an array which contains all the nodes that you want computed. Here, it is y and y_. We've used the fetches parameter before now as well, except that we haven't explicitly named it as fetches. The fetches parameter always holds the node that you want to compute when you pass it to Session. run. You can see examples of Session. run statements here, some of which use the fetches parameter by specifying it explicitly, and others do not specify it explicitly. Make sure your feed dictionary has all the input placeholders specified for calculating both y and y_. Run this bit of code, and you'll notice that we get two results in one Session. run statement, seen at the very bottom. The first result is for y, and the other is for y_.

Variables

In this clip, we learn about another important TensorFlow construct, the variable. Variables in TensorFlow are constructs which allow you to change the value that is stored within them. Some time ago, we had spoken about how machine learning iteratively arrives at the best possible solution. Regression here is used as an example of a class of machine learning algorithms called supervised learning algorithms, which perform multiple iterations before they arrive at the final model. These models, when represented in TensorFlow, use variables to store the values that change as the model converges. In the case of finding the best fit regression line, where a line has an equation y is equal to A plus Bx, we start off with some values for A and B. These values can be Astart and Bstart. We then see how good a fit this particular line is by calculating the error from our input data point to that line. This is the least square error. Based on the value of the least square error, we feed back to our original values of A and B, tweak them a little bit to get new values. Our objective is to minimize the error between the regression line and the points in the input data set. By constantly adjusting the values of A and B, we'll finally get to a line where the error is minimized. This is the best fit line and is called the regression line. In order to calculate this best fit line, we have to have the ability to constantly change the values of A and B 'til a final result has been achieved. And this process is common across a whole class of machine learning algorithms. These algorithms iterate to get closer to the final solution, which means our model should have the ability to hold values which are constantly updated or changed. So far, we haven't seen a TensorFlow construct which can do this. We've seen constants which are immutable. The values do not change once they are assigned. We've seen placeholders, which are used to feed in the input data. Once the data has been fed in, placeholders are assigned, and their values do not change after that. What we really need are variables. These are constantly recomputed as the graph is recomputed in TensorFlow. Variables are mutable Tensor values that persist across multiple calls to Session. run. They hold their value 'til they're explicitly updated. In this demo, we'll see how variables work and how they're different from constants and placeholders. In this demo, we'll start off at the same computation that we saw before, y is equal to Wx plus b, but this time, W, as you can see here on screen, as well as b, will be variables. Every variable is assigned an initial value, and here, W has initial value 2. 5 and four. The Tensor is of type float32. As for the other Tensors involved in this computation, x is a placeholder, again, of type float32. b is once again a variable with an initial value of five and 10. And to y we assign W multiplied by x plus b. If you're using variables in TensorFlow, you need to have your session explicitly initialize these variables. Call tf. global_variables_initializer. This is important because you can't use variables in your program without first initializing them. global_variables_initializer will initialize all variables that you've declared. This is also a computation assigned there to init. This is another computation node which needs to be executed to initialize your variables. Use the Python with statement to instantiate and assign a session, and you need to call Session. run with init in order to initialize your variables. If this initialization step is not run, your computation will throw an error. Once your variables have been initialized, you can compute your graph like you did before, using Session. run and specifying the values of your placeholder within the feed dictionary. You can switch over to your terminal window and run this bit of code, and you get a result. We haven't used the variables here any differently than we used the constants earlier. Don't worry; we'll get to that in just a bit. Before that, let's see how we can initialize just a single variable, rather than all the variables in your program. Here, we want to calculate an intermediate value, s is equal to W multiplied by x. This computation only requires the variable W, so you can call tf. variables_initializer and specify those variables that you want initialized within an array. Here, it's just W. Call Session. run to initialize the W variable. Within this instance of your session, b has not been initialized. Only W has, which means, if you try to compute y, which requires both W and b to be initialized, you'll get an error. Let's check this out in the terminal window. Run this Python code, and there you see it. We have a FailedPreconditionError. The message there very clearly says, "Attempting to use uninitialized value var_b. " Well, we'll comment it out, since this clearly doesn't work, and we'll only run our computation on s, which requires the variable W, which we have initialized. Now, if you run this program, it'll run through without an error and produce the results that you expect. Now, let's actually use variables like they're supposed to be used. So far in this program, we set up and initialized variables, but we haven't actually updated the values of these variables once they've been initialized. In this example, we'll see how variables can be updated. We have a number and a multiplier. The number is initialized with two, and the multiplier's initialized with the scalar one. These are both variables. We'll initialize these variables by calling the global_variables_initializer. We can assign a new value to a variable by using the assign method. number. assign will assign a new value to the number variable. This computation simply takes the result of tf. multiply, that is, number multiplied by multiplier, and assigns it to number itself. This assignment computation is stored in result and can be executed using Session. run. result is a computation node just like any other in TensorFlow. Let's calculate this result a number of times within a for loop. Instantiate a session, initialize the variables, and within a for loop that runs 10 times, compute the result. The result computation is simply the number multiplied by the multiplier and assigned to number itself. Number is a variable, as is multiplier. In every iteration, we also increment the multiplier by one. This we do using the assign_add statement. So each time we execute result in the next iteration, the value of multiplier has been incremented by one. Run this program, and you'll see how variables work. The first time we multiply number and multiplier, we get the result two, because two multiplied by one is equal to two. We increment the multiplier, and the multiplier's new value is now two. Two multiplied by two is equal to four. The new value assigned to number is now four, and so on. These are just example computations to show you that values stored within variables can be updated during the course of a program.

Default and Explicitly Specified Graphs

The computation graph set up by TensorFlow uses edges to represent Tensors and nodes to represent operators. But is there just one graph that we can set up per program? We can explicitly define as many graphs as we need within a single TensorFlow program. In this demo, we'll see how. Every TensorFlow program has a default graph associated with it. Any computations that you perform, any placeholder, variable, or constant that you instantiate fall into this default graph. In all the programs that you've written so far, you've been implicitly using this default graph. You can also logically segment the graph that your program creates by instantiating a graph explicitly using tf. Graph. You want your Tensors and operators to be added to this graph, you have to set it as a default before you specify your computations. This you'll do by using the g1. as_default method. I'm going to go ahead and instantiate a session under this. All computations and Tensors that I now instantiate will belong to the graph g1. Within g1, I'm going to perform the y is equal to Ax plus b computation. A and b are constants, and x is a placeholder. I can call Session. run to execute y and pass in the values of x in my feed dictionary. To ensure that all these Tensors and computations are within the graph g1, I can use in a third statement. The y operator has a graph object associated with it. I can check whether it is g1. When you run this Python file, you'll find that it runs through without an error indicating that the assertion did not fail. The graph that the computations were added to was indeed g1. Within the same program, we'll instantiate another graph, g2. If you use g2 as the default graph, all the computations that we add within the g2. as_default block will belong to g2. Here, we simply calculate A to the power x, where A is a constant, and x is a placeholder. Use Session. run to compute y here, as usual. Now, we'll run an assertion statement to check whether y. graph is g1. It should not be. This assertion should throw an error. All the computations here have been added to g2. Run this code, and you'll see the assertion error. Go back to your editor and fix the assertion to be y. graph is g2, and now the program should run through successfully without failures. When you don't explicitly specify a graph in your TensorFlow program, all your computations are added to the default graph. You can get a handle to the default graph using tf. get_default_graph. All the computations that we're going to add now will be added to the default graph. Notice that we don't have a with statement indicating that the computation should be added to another graph. We can check whether the computations have been added to the default graph using this assertion. Run this bit of code, and you'll see that it runs through just fine.

Named Scopes

TensorBoard is an extremely useful visualization and debugging tool, but as your computation graph explodes in size, you need some way to abstract away the details and get the bigger picture. We want to go from a graph which looks like this, where we are overwhelmed by the details, to a graph which looks like this. This gives us a big-picture understanding of the logical components in our program and allows us a zoomed-in view when we need it. You can do this with just a little bit of additional work in your code by using named scopes. Let's say you want to represent a bunch of equations using your graph. The first one is y is equal to Ax squared plus Bx plus C, and the second equation is y is equal to Ax squared plus Bx squared. We'll first set up values for the constants A, B, and C. Make sure you specify the right names so we can view it on TensorBoard. x is a placeholder whose value we'll feed in when we evaluate these equations. Set up the computations for the two equations. This block is for y is equal to Ax squared plus Bx plus C. We store the result of this equation in y1. In TensorBoard, if you want to identify the Ax squared that is part of this equation from the Ax squared that will be part of the next equation, we set up different names and variables for it. The next block of code represents the equation Ax squared plus Bx squared. Set up the intermediate calculations for this equation and store the result in y2. Once again, I identify the Ax squared that is part of this equation with a different variable and a different name on TensorBoard. I'll add in a final computation, y, which is equal to y1 plus y2. I want to calculate y, which I do using Session. run, and write out the graph to TensorBoard. When I run this program, everything runs just fine. This program didn't really teach us anything new. The interesting thing lies in the TensorBoard graph. Run TensorBoard and view the graph, and you'll see that it's really complicated. There are a whole bunch of computation nodes, lots of Tensors which flow through them, and debugging this or even viewing this and figuring out the logic here can get pretty complex. We really need a way to organize things in TensorBoard, and this we can do using named scopes. What would be really nice would be to have each of these equations in its own scope, own little box, which we can view in TensorFlow. This we can do by using the with tf. name_scope method. Specify the name of the scope as an argument to this. Here, it is Equation_1. Similarly, set up the name scope for Equation_2, and the final result, which is y1 plus y2. That's called Final_Sum. We have three named scopes in this program. The first two named scopes feed into the third. Think of these named scopes as logical blocks of code that you might want to debug separately in TensorBoard. Because they're now set in different blocks of code, you can use the same variable Ax2 to reference Ax squared. Because the same variable name is in different named scopes, you'll find that it is easier for us to identify it in our TensorBoard graph. Let's run this code and then switch over to TensorBoard and see how this graph looks now with named scopes. This is how the graph looked earlier. Let's refresh TensorBoard, move over to the GRAPHS tab, and here it is, our graph with named scopes. See how easy it is to see the logic that exists in our equations and the final sum? You can use the plus sign to expand individual scopes to see the computations that are done within it. Named scopes let you logically group your computations and see how data flows. When you work on real machine learning programs with thousands of nodes, named scopes are an extremely handy tool to manage how your graph looks and to enable debugging.

Interactive Sessions

Having to reference a session object each time we want to execute something can be cumbersome. TensorFlow provides a way around this by allowing you to use something called interactive sessions. Interactive sessions allow you to work with a session in TensorFlow without holding a reference to the session instance. We are in the Python triple environment. We've instantiated an interactive session and stored it in the sess variable. Storing it in the sess variable is not required if you're using interactive sessions. Let's set up a bunch of simple computations. These computations can be basically anything. The final result is in y. If we want to evaluate y, we do not need to specify the session. We can simply run y. eval, and it'll automatically pick up the default interactive session. Running y. eval with an interactive session is the equivalent of tf. get_default_session(). run(y). Interactive sessions function exactly like regular sessions except that it makes writing TensorFlow code much less cumbersome. This is especially true when you're using the Python interactive environment. And with this demo, we come to the very end of this module. We started off by understanding how regression works as a machine learning problem, and we used the example of regression to understand why placeholders and variables are important constructs in TensorFlow. We also saw how we can make TensorBoard visualizations more useful using named scopes. In the next module, we'll work with images. TensorFlow is widely used for image recognition problems, and learning how images are represented as Tensors and working with them is an important skill.

Quick Overview: Linear Regression in TensorFlow

This course does not cover regression in TensorFlow. There are other courses on Pluralsight that you can view in order to learn regression models in TensorFlow, but we use regression a lot in our machine learning explanations, so I felt it made some sense to end this module with a quick overview of what regression code in TensorFlow looks like. This is a linear regression and implemented in the simplest possible way. This is a standard example taken from Google's documentation on TensorFlow. In the very first step, you'll set up the model parameters. The model parameters that we want to find for linear regression is the value of the slope, W, and the value of the y-intercept, b. These are variables in our TensorFlow model. The input is the x value that we feed in, and the output that we want from our linear model is the predicted value of y. We want to feed in a range of values for x in our training data. x is a placeholder. Since this is a linear regression, the actual model that we want to represent is of the form linear_model is equal to W multiplied by x plus b. Initialize a placeholder y which will hold the output labels of the training data. These are the y values corresponding to the x values of our training data, and this is what our machine learning model will use to train itself and get the right values of W and b. Linear regression involves finding those values of W and b which give us the best fit line. The best fit line is one which minimizes the least square error. This is the code to calculate the least square error set up by our model, and the label y values that we passed in. While building machine learning models, you often use an optimizer which allows you to minimize your loss. The most common optimizer used is the GradientDescentOptimizer. An explanation of the GradientDescentOptimizer is beyond the scope of this particular course. If you're interested in building regression models in TensorFlow, you should check out the course that's dedicated to that. While training our machine learning model, we set up our GradientDescentOptimizer to minimize our loss function. Set up the training data, the x and y values that you're going to feed into our regression model. Set up a session and initialize the variables in this program. These are standard, boilerplate steps. Training data is typically passed in batches through the model several times. Here, we don't have batches because we have just four data points in our training data. We simply pass it through the model 1000 times. This is just an example, after all. And finally, we use Session. run to evaluate the final values of W, b, and the final loss value after our training is complete. This is the loss that our GradientDescentOptimizer minimizes. Print these values to screen, and then let's run the code and see the output. Here are the values of W and b from our linear regression model which best fit our training data. These are the values which minimize the loss function. This was a quick overview of linear regression implemented in TensorFlow for the sake of completeness. We've spoken so much about how linear regression works, it made sense to include some code to give you a glimpse of how a real-world program in TensorFlow is implemented. And with this demo, we come to the very end of this module. We started off by understanding how regression works as a machine learning problem, and we used the example of regression to understand why placeholders and variables are important constructs in TensorFlow. We also saw how we can make TensorBoard visualizations more useful using named scopes. In the next module, we'll work with images. TensorFlow is widely used for image recognition problems, and learning how images are represented as Tensors and working with them is an important skill.

Working with Images

Image Recognition and Neural Networks

Hi and welcome to this module on working with images in TensorFlow. Neural networks are a class of deep learning algorithms that perform really well in image recognition. This means that understanding how these images are represented in TensorFlow and working with these images become very important. We start off by seeing how we can represent both color and gray-scale images as tensors. We'll see how we can read a whole number of images from our local machine and implement image operations such as transpose, resize and cropping. Let's see a logical overview of how image recognition works. Any image on your computer if you zoom in far enough you'll notice that they're made of millions of dots. These are pixels. Pixels in an image hold information about that portion of an image, either color, saturation, hue, intensity and so on. All these millions of dots come together to make the image that you see. Image recognition is a very complicated machine learning algorithm, and does not not occur in a single step. You feed in images that are represented as pixels, you have a number of steps which identify edges in the image, may identify colors and shapes and finally, you recognize the image for what it is. Let's say it's the photo of a horse. As we learn more about applying machine learning techniques to recognize images, neural networks, specifically convolutional neural networks work very well for hard image recognition tasks. We won't go into what CNNs are in this beginners class but TensorFlow is a great library for building convolutional neural networks. Earlier in this course we briefly spoke about how neural networks are built up and can be used for image recognition. You have a whole corpus of images which you feed into the input layer of the neural network. Neural networks are examples of representation machine learning algorithms where the algorithm itself is also responsible for finding the interesting patterns in data. It does feature selection as well. These corpus of images go into the input layer and any layer which you as a user have interaction with is the visible layer. The remaining layers each perform a specific task. One might recognize edges, one might distinguish corners in the image. In the final layer puts all this information together to identify object parts. This kind of processing using layers working on subsets of the patterns can be generalized to any neural network. Each of these layers can be made up of hundreds, even thousands of neurons. Neurons are the active learning elements which understand and recognize patterns in machine learning. TensorFlow's representation of the real world as a computation graph where edges are tensors transformed by the nodes that are operators is great for representing neural networks. TensorFlow is optimized for building neural network solutions for image recognition.

Representing Images as Tensors

In this clip we'll see how images are represented in TensorFlow. Understanding the representation will help us understand the operations that we perform on images. Here is an image and we'll use a tensor to represent it. What information does the tensor have? Every image is made up of a pixel. You can imagine the pixel as forming a grid over the image. This can be represented by a two-dimensional array. Each pixel here holds some value based on the kind of image it represents, either a color image or maybe a gray-scale image. In a color image every pixel is an RGB value. R stands for how much red, green and blue is present in that particular pixel. In a color image every pixel requires three values to represent the information within it. A value for R, G and B and each of these values will be in the range zero to 255. RGB values are something that you're probably familiar with. In order to represent red, the R value will be 255, G and B will be zero. In order to represent green, R value will be zero, G value will be 255 and B will be zero once again. And similarly for the color blue, B will be 255, the remaining two will be zero. By tweaking these RGB values there are a whole host of colors that you can represent. One pixel, three values for RGB. This is in a color image. The number of values required to represent a pixel is the number of channels that an image has. A color image has three channels. The same matrix representation of pixel values can be used to represent gray-scale images as well. Gray-scale images contain shades of gray ranging from white up to black but no color. In a gray-scale image every pixel contains exactly one value. The pixel represents the intensity of that particular pixel and that ranges from zero to one. You might also find gray-scale images where every pixel value is represented by larger integers. They also represent intensity. Using values in the range zero to one is more common though. A pixel value representing a gray-scale image might contain a value such as 0. 5 which refers to its intensity. When you have one value to represent a pixel, the image is said to have one channel. TensorFlow can be used to represent both single channel as well as multi-channel images. If you think about it the grid representation of the pixels in an image is nothing but the two-dimensional matrix. The dimensions are the length and the width of the image that you want to represent. Representing images however, requires one additional dimension, the dimension which represents the value for each pixel. The number of elements that exist in this third dimension depends on the kind of image that you want to represent. In a gray-scale image the third dimension has just one element for the single channel and in the color image the third dimension has three elements to represent R, G and B. If the matrix that you see on screen is a six by six matrix representation of an image, the first two dimensions are six and six, the third dimension is one for the gray-scale image on the left and the third dimension has three elements for the color image on the right. Tensors are nothing but end dimensional arrays which means these images can be represented as tensors very easily. One image will be a three-dimensional tensor.

Transposing Images

In this demo we'll read in an image into TensorFlow using the matplotlib library and then perform a transpose operation on this. Matplotlib is a very useful plotting library for Python and can be used with NumPy as well. I call pip install matplotlib to download the library. Because I already have it on my machine, I get the message which says requirement already satisfied. If you don't have matplotlib already installed, this will download and install matplotlib on your local machine. You might find that matplotlib doesn't directly render images on your Mac. It says it doesn't know which backend to use to render these images. In order to get this to work you may need some additional setup. Open up the matplotlibrc file. It should be located on the path that you see on screen. Specify the backend that it should use to render images. In this case we are going to use TkAgg. Matplotlib alone can only deal with PNG files. In order to be able to deal with JPEG files as well you need an additional library called Pillow. If you have an older version of the Python image library, PIL, on your machine, you need to uninstall it and then install Pillow. Here is a Stack Overflow link which shows you how that can be done. Once you've uninstalled PIL, you can install Pillow. Just run pip install Pillow in order to get the libraries on your local machine. I already have Pillow installed which is why I see this message. In this demo we are working with TensorFlow as well as matplotlib, so set up the correct import statements for all we need. I'm going to read in a color image of a dandelion which I got from Wikipedia, Greg Hume is the photographer and it's a really lovely color image. Make sure you specify the complete file path to the image you want to read. Use the imread command in the matplotlib image namespace in order to read in this file. The image variable holds the resultant image as a NumPy array. This array has a shape that we can print out. We can also print out the entire contents of the array. You can also view this image by calling plot. imshow and then plot. show. This will display the image to your screen. We haven't performed any TensorFlow operations yet. Let's just run this Python code and see what the result is. There you can see it that the image has shape 960 by 1280. That is the width and the height of the image and the third dimension has three elements, it's a color image. You can also see in the image array that every pixel is represented by three values, R, G and B. Matplotlib has also rendered the image to screen. Now for the TensorFlow stuff. Set up X, a TensorFlow variable which holds this image as a tensor. Use the global variables intializer so that all variable are initialized and kick start a session where we'll run the image operations. We'll run a transpose operation to flip the width and the height of this image. We'll use a general purpose transpose command that TensorFlow provides which can work on any end dimensional matrix. This is the tf. transpose and it can basically flip around the axis of any matrix in any order that you want. Notice the parameter perm which we pass in as one, zero, two. This has special meaning. The original axis indices for the matrix representing the image are zero, one and two in that order. When we perform the transpose, we are swapping the first and second axes at index zero and one. The new order of the original axis is one, zero, two and that's what we've specified in the perm parameter. This swaps the width and the height of the image but leaves the third axis, the values representing the pixels as is. Call session. run on this transpose operator, print out the shape of the result and display it using matplotlib. Switch over to the terminal window and run this Python code. First it'll display the shape and the array representing the original image and the image itself. Once you close that image, here is the transposed image. Width and height have been swapped. You can verify this using the image shape that you printed to screen. The original image was 960 by 1280 with three channels because it's an RGB color image and the final shape is 1280 by 960. The width and height have been swapped. The number of channels remain the same. Instead of using the tf. transpose method which is not image specific, it can work on any tensor. We can instead use a transpose method that is meant especially for images, tf. image. transpose_image. Notice how much simpler this API is. You do not need to specify the new order of the axis. It'll simply swap the width and the height of the image while leaving the channel unchanged. If you run this code you'll find the result is exactly the same as we saw earlier. Here is the first image, you close this, you'll see the second image and the width and the height have been swapped.

Resizing Images

In this demo instead of one image, we'll work on multiple images together. We'll read in a list of images using a queue and coordinators which manage multiple threads which add to that queue. We'll then resize all to these images to be the same dimensions and show image summaries in TensorBoard. In both TensorFlow and image from the PIL library. here are the list of images that I have stored on my local machine. They are JPEG images and I have an array set up with their names so I can use these to read from these files. It so happens that I've chosen all black and white images but these images are not gray-scale. They still have three channels for representation. They are RGB images but in black and white. I'm going to use the queues construct in TensorFlow in order to read these files. Tf. train. string_input_producer takes in all the strings within our original_image_list and creates a queue of these file names. I want to read one file in its entirety so I use the tf. WholeFileReader. Instantiate a session to execute our TensorFlow operations. A session object in TensorFlow is multi-threaded and we want to use these multiple threads to read in our image files. TensorFlow provides a coordinator class which you can instantiate. This makes working with multiple threads and queues very, very easy. A coordinator abstracts away all queue-related threading activities. All threads must be able to stop together. Exceptions need to be caught and reported, queues needs to be closed properly. All of these are handled by the coordinator. Queues are a very convenient TensorFlow way to compute tensors asynchronously using multiple threads. Let's kick start all the threads within our queue using start queue runners. I store all the image tensors that we read in in image_list using a for loop to iterate through the number of images that we have so I can de-queue from the queue that I have set up. Calling the read method on our WholeFileReader will read the entire file that is at the head of the queue. This returns a tuple. The first field in the tuple is the file name which we ignore as specified by the underscore that you see on screen. The next field is the actual image file. This we store in image_file. TensorFlow has a number of helper functions which allows you to decode these image files as tensors. Here we use decode_jpeg because it's a JPEG file. You can also use decode_png etc. The image is a reference to an image tensor. Let's resize this image before adding it to our list. Use the tf. image. resize_images and pass in the image tensor. Also specify what dimensions you want this new image to be. Here we want the image to be 224 by 224. Call set_shape on the image tensor and set its width, height and number of channels, 224 by 224 and three channels. So far we've only built up the computation graph. Now we need to execute it in order to actually resize the image. Call sess. run on image and it'll perform the image resize. I'm going to use the Pillow library to display the resized image on screen so you can see what it looks like. Use the image. from array, pass in the image array but you need to convert its elements to be of type uint8. Call the show method to display this image. What we have so far is a three-dimensional image tensor. I'm going to expand the number of dimensions this image tensor has before I add it to my image list. I want every image to be represented as a four-dimensional image tensor where the first dimension indicates which image I'm referencing. So, if you have a series of images, the first dimension points to the image that you're referencing and the remaining three dimensions describe that image. This kind of representation is very useful when we have lists of images. Having a separate image for each tensor is clunky. A single tensor representing multiple images is also useful when we want to perform a batch operations on all of these images in one go. The expand_dims method that you see on screen adds one more dimension to our image tensor. The coordinator has kick started all the threads to read in the various files. I now want to finish off the coordinator's execution. I call coordinator. request stop which will stop all the threads and I wait for all threads to complete by calling coord. join. We'll now see how to write out image summaries in TensorBoard. I'm going to give each image file a new name. I initialize an index to zero. Instantiate the file writer as usual. Use a for loop to iterate through every tensor we have in our image list and then write out a summary statistic for every image. This is the summary string which we create by calling sess. run and performing the tf. summary. image operation. A summary string is just a summary representation of an image which we will show in TensorBoard. Pass in the image tensor and give the tensor a name. The names for the images will be image-0, image-1 and so on. Add this summary string that we've generated to our file writer and implement the index to deal with the next image. Close the summary writer. We are now ready to run this bit of code. Run this Python program and there you'll see it, the shape of our resized images. Each image is 224 by 224 with three channels. The Pillow library also renders all these images to screen. Notice that they're all now of the same size. But I started off with four images and I can only see three here. I did a little hunting and one of the images was hiding behind my terminal window. Let's run TensorBoard and see if these images are available in our summary statistics there. Here is the graph of what we've just implemented in TensorFlow. Notice that as we perform more complex operations with TensorFlow, our graphs are also getting steadily more complicated. Now if you switch over to the images tab, you'll actually find stuff there. Notice the images that we've written out, images zero through four.

Representing a List of Images as a 4D Tensor

We just spoke about images being represented as four-dimensional tensors. In fact, TensorFlow often deals with lists of images, not just a single image specifically. Lists of images can be represented as a four-dimensional tensor. For the images to be represented as one tensor in 4D, all images need to be of the same size. A list of images that every image has different dimensions and different number of channels is typically not represented as a 4D tensor in TensorFlow. Let's say that you had a list of images. This is what a typical 4D tensor would look like. Let's start off with the number in the fourth dimension. This represents the number of channels that are needed to represent that image. For color images we need three channels. Moving towards the left, the next two dimensions are the height and width of each image in the list. All images in the list should have the same height, width and number of channels. And finally, the first dimension is the number of images in that list. The very first image is at index zero, the second image is at index one and so on. In this demo we'll perform some other transformations on images such as flip and crop. We'll pack a list of images into one 4D tensor and display this list of images on TensorBoard. We'll start off with the same code that we saw in the last demo. We have read in a list of images from file using a queue and a coordinator which manages the threads in a queue. We then read from individual files, decode each file as a JPEG image, resize each image, converted it to a 4D tensor by expanding its dimensions and finally, added it to an image list. Let's make changes to this code. First I'm going to delete the image. from array which simply displays the images to screen. We don't need that anymore, we can simply view the images in TensorBoard. Image_array is simply an image array. We want to convert a NumPy array to a tensor. Tf. stack is usually used to convert a list of rank-R tensors into one tensor which is of rank R plus one. Here since we have just a single image and not a list of images, Stack simply converts this to a tensor. The list of images that we had earlier will simply now hold a list of image tensors. Let's delete all the code that has to do with the summary writer here and we'll run this bit of code and see what output it provides. When you run this code you'll see that now each item in the list is a tensor. It has shape 224 by 224 by three where each element is of type flow 32. Now, let's run the Stack command on this list of image tensors that we've created to get a single tensor which is of four dimensions. Tf. stack on an image list will convert a list of images which are 224 by 224 by three pixels to a single four-dimensional tensor. The first dimension indicates the number of images in the list. You can see that it makes sense when you run this bit of code. Notice the very last tensor there. It has a shape four by 224 by 224 by three. There are four images in our ist. Having the images set up as a single 4D tensor makes writing out summaries for TensorBoard much easier. We set up the summary writer as before and create the summary string. In the summary string we run tf. summary. image on the 4D image tensor representing a list of images. We add the summary to our summary writer, close out the writer and then run this code. Nothing new to see here on the terminal screen. The real changes are in TensorBoard. Here in TensorBoard instead of having a separate name for every image, which was pretty cumbersome to write out and code, we have the images tensor where all the images are listed. One thing you might notice here is that there are just 3 images, but we actually wrote out 4. This is because the default number of images that our summary writer writes out is 3. You can change this default by passing in the max_outputs parameter in our tf. summary. image as you can see here on screen. If you want 4 images to be rendered on TensorBoard specify max_output =4. Let's quickly take a look at some other image operations that we can do. We can call flip up and down on an image and have it turn upside down. Run the code as before and then view these images in TensorBoard and you'll see that each of them have been flipped upside down. Or you can call central_crop which allows you to crop a portion of your image. The fraction of the image that you're interested in is specified in the central_fraction parameter. Here we are interested in the center of 50% of our image which is why we specify 0. 5. Run this code. Notice that with central_crop which retain the center 50% of every image, each image now has dimensions 112 by 112 as its length and width. View the images in TensorBoard and you'll see that the images seem zoomed in. We've central cropped our upside down images to focus only on the center 50%. And this brings us to the end of this module where we worked with images. We understood how images are represented in TensorFlow, both color as well as gray-scale images. We performed a bunch of transformations on images so that we have some practice working with them such as resize, flip and crop. And we also learned how we can work with multiple images in TensorFlow and represent a list of images as a tensor. In the next module we'll work on the MNIST handwritten digit dataset and apply a real machine learning algorithm that came nearest in order to identify handwritten digits.

Solving Basic Math Functions

The MNIST Dataset

Hi and welcome to this module where we'll be using TensorFlow for a simple machine learning algorithm. We'll use the K-nearest-neighbors algorithm to recognize handwritten digits which are present in the form of images. In the course of this module, we introduce the MNIST handwritten digit dataset. It's a great dataset to get started with on machine learning and pattern recognition techniques. We'll then understand how the K-nearest-neighbors machine learning algorithm works before we apply it to this MNIST dataset. And finally, we'll implement the K-nearest-neighbors algorithm in TensorFlow and use it to identify handwritten digits from zero to nine which are part of the MNIST dataset. MNIST contains a large number of images where each image represents a handwritten digit. All these images are pre-processed and well formatted which means it's very easy for you to use in your machine learning algorithms. MNIST stands for Modified National Institute of Standards and Technology. The MNIST dataset is freely available at Yann LeCun site. You see that it has a training set of 60, 000 examples and a test set of 10, 000 examples all images with handwritten data. Every image has a label associated with it which indicates the digit that is represented by that image, zero through nine. You can see the GZ files which contain the training set, the test set and the corresponding labels right here on screen. We won't be downloading these files from here though. We'll just leave it to a TensorFlow library that will do that for us. This is how the images in the MNIST library look. Each digit is in gray scale which means it has just a single channel. Every image has a standard size, every image is of size 28 into 28, 28 pixels width and 29 pixels height which gives us a total of 784 pixels to represent one image in the MNIST dataset. You can imagine every image as being subdivided into a grid where different cells of the grid hold one pixel value. Every pixel holds a single value for the intensity of that pixel. Remember, this is a single-channel image. Let's see how the values for a particular image might be laid out. If you look at the numbers on screen, you can see that the portions where there are strokes of the image have higher intensity values. The white spaces in the image have lower intensity values. The intensity values should give you an idea of what that digit is, so if you kind of look really hard, you can see a four right there in that grid representing that image. Each image in the MNIST dataset has a label associated with it which tells us what digit is represented by that image. So, each of these images on screen will have the labels five, zero, four, one. Using the MNIST dataset is a great way to start off with machines learning and pattern recognition techniques. It's the equivalent of the Hello, World program when you start off with programming languages.

The K-nearest-neighbors Algorithm

In this clip we'll understand the K-nearest-neighbors machine learning algorithm which we are going to use to identify handwritten digits. Before that, we'll talk about machine learning algorithms in general for a little bit of context. Machine learning algorithms can be divided into two broad categories. These are supervised learning algorithms. Here the training data that you feed into your machine learning has labels associated with every elements. These labels are used to correct the algorithm which is then fed back to get better prediction. Regression which we saw earlier is an example of a supervised machine learning algorithm. The other category is the unsupervised machine learning algorithm. The model has to be set up right but the model is responsible for understanding the structure and patterns that are present in data. There is no training dataset and no labels that are associated with the data to correct the algorithm. I take you through an overview of how supervised learning works. Let's assume that the input variable into the algorithm is X and the output corresponding variable is Y. Y is the label that you have available for every element in your dataset. The objective of a supervised learning algorithm is to find the mapping function such that Y is equal to function of X. What does this function that generates Y from X? The goal is to approximate the mapping function so well that when new data comes in when you have new input data X you can predict the output variables Y for that data. Because we have three label data available to us for our input of X, we have the corresponding labels Y, we'll use this existing dataset to correct our mapping function approximation and get better approximation as a result. In unsupervised learning we have the input data that's represented by X but we have no corresponding output data available for this set. The goal of the algorithm is to model the underlying structure in this data, find patterns within it in order to learn more about it and make predictions. The machine learning algorithm here is kind of left to its own devices with no supervision. Algorithms have to self-discover the patterns and structure in the data. We've mentioned the importance of training data in supervised learning models. Unsupervised learning does not have training data. In the ML-based classifier that we saw earlier the corpus of data that you pass in to generate this ML-based classifier is your training data. Training data is required in supervised learning techniques. The K-nearest-neighbors is a supervised learning algorithm which uses this training data to predict values for the input dataset. It'll try to find what element in the training data is most similar to the current sample. The K-nearest-neighbors machine learning algorithm uses the entire training dataset that you made available to it as its model. Every element in this dataset is associated with a label, for example, the elements here will be labeled with what these images are such as rocket, buildings, pig and so on. This training dataset is what we'll use to make predictions about any new data point which comes in. Let's say a new image comes in and we want to predict what this is an image of. The K-nearest-neighbors algorithm may find out which element in the training data or which image in the training data this particular image is the closest to or the most similar to. It will try to find the nearest neighbor of this particular image. How do you define similarity or the nearest neighbors? There are a whole number of ways in which you could do that. We'll see an example of that in the next clip but for now, imagine that we're comparing this image of a house with each of our images in the training dataset, is it like a building? Not really. Is it like the signal or pig? Not really. Provided your comparisons for similarity have been set up correctly, you're likely to get the result that this house looks very much like a shop, K-nearest-neighbors will find that element in your training dataset that is the most like the sample that you're trying to evaluate. This is how the algorithm works logically. The question now arises how do we calculate the neighbors of a sample? How do we say that this bit of data is close to this other bit of data? We do this using something called distance measures. Distance measures indicate how far away one data point is from another. There are a whole host of distance measures that you can use to calculate the distance between data. These distance measures don't just apply to coordinate geometry, they can also extend to images because images are nothing but matrices with numbers representing the pixel values. You might have heard of some of the most common distance measures such as Euclidean distance, Hamming distance, Manhattan distance etc. Distance measures are an important part of the K-nearest-neighbors algorithm because that determines who your nearest neighbor is. We'll cover distance measures in a little more detail in the next clip but before we're done talking about K-nearest-neighbors, let's visually understand this concept. Imagine a two-dimensional plane and you have a whole number of points on this plane. Imagine that each point represents an image. This is your training dataset. Now, let's say some test data comes in and you want to find the nearest neighbor for this S data. Into which of these clusters will this image fall? You'll calculate the distance of this image from all its neighbors and find the nearest ones. If you find that the K-nearest-neighbors in this case, KS three are blue points, it's safe to assume that this is a blue point as well. Let's say we have another point in our test data. We calculate the distance of this point from all other points and find that the nearest points are all red. That case we can conclude that this point is red as well. This in essence is what the K-nearest-neighbors algorithm attempts to do.

L1 Distance

In the last clip we mentioned a number of ways in which we can calculate the distance between any two points. These are the distance measures. The one that you're most probably familiar with from high school math is the Euclidean distance. The Euclidean distance has a standard formula which you can use to measure the distance between any two points in space. The Euclidean distance gives us the shortest distance between two points as the crows flies, so if you have two points, a purple and a green one as you can see on screen, the Euclidean distance is the distance of the line which directly connects them. Euclidean distance makes sense when you have points in continuous space but if your points are discretized there is another measure that you can use. The L1 distance is used to measure distance as discrete steps, it's not as the crow flies. When working with our gray scale images from the MNIST dataset, we won't use Euclidean distance to find the nearest neighbors. We'll use another distance measure called the L1 distance. This L1 distance is known by a number of names, L1 distance, it's also called snake distance, city block distance or Manhattan distance. Let's say you wanted to get from the purple block to the green block. Then you can imagine that you step through the grid in this way, follow these arrows to get from purple to green. This distance covered by these arrows is the L1 distance between the purple and green blocks. You can see that the L1 distance is longer than the Euclidean distance because Euclidean distance is as the crow flies. The L1 distance is also more intuitive and easier to calculate. Once you look at the layout of these arrows, the various names for L1 distance make more sense. Cities could have their blocks laid out in a grid pattern, definitely true for Manhattan. Let's add some coordinates here to this grid black. The purple block is at coordinate one, zero and the green block is at five, four. To calculate the L1 distance, first consider the X coordinates. These are five and one. Subtract one from five and we get the value four. Now take the corresponding Y coordinates, these are four and zero, subtract zero from four and you get the value four again. Find the absolute values of the differences between the coordinates and add them up. You get eight. Eight is the L1 distance between the purple and green blocks. Another way to see this is by counting the arrows. You'll see there are eight.

KNN in TensorFlow

Now that we are familiar with the MNIST dataset and have understood the K-nearest-neighbors algorithm, we'll see how we can use K-nearest-neighbors in order to recognize handwritten digits. We'll use the L1 Distance measure to find the nearest neighbor and then measure the accuracy of the algorithm on our data. We'll break up this algorithm into three logical steps. First step involves accessing and downloading the MNIST images which we'll do using TensorFlow libraries. This will have both training and test data. The next step is to set up the formula for L1 distance between test digit and the training dataset. We predict the value of the test digit by finding that image in the training dataset which is its nearest neighbor. In this algorithm we'll use K is equal to one, just the closest neighbor and finally, we'll run this algorithm and make predictions for our entire test dataset. This will also help us calculate the accuracy of our algorithm. Let's start off by accessing the MNIST data. Before I start with the code, I should give credit where it's due. This is code written by Aymeric Damien and put up on GitHub here at this URL. It's a great example to start off with for TensorFlow beginners. Make sure you explore this site when you get a chance. It has tons of very interesting TensorFlow examples. Set up the usual imports for numpy and TensorFlow. We also want to use the MNIST dataset which we can get using a library within TensorFlow. This we'll do using input_data from the TensorFlow MNIST tutorials namespace. Input_data. read_data_sets is a direct way to access the GZ files which has MNIST data which we saw in the Yann LeCun site. The MNIST variable here will contain the training, validation and test datasets allowing us to access this information in batches. The first argument to read datasets, mnist_data/ is the directory where we want the MNIST G zipped files to be stored. The second parameter, one_hot which is set to true, defines how we want the labels associated with every image to be represented. This will have the labels as a 10-element vector. This vector will be comprised of all zeros having a value of one only at that index which represents the digit. For example, the digit four will have value of one at index four in its output label. Let's understand this one_hot notation visually. We've seen earlier that this how the images within the MNIST dataset looks like. Every image is a standard size of 28 by 28 taking up a total of 784 pixels. This pauciarticular image has a value of four and this is represented by a vector with N elements. Each element is zero except for one element at index four. This is the one_hot notation for the labels of the MNIST dataset. If you're representing an image with the digit five, the vector will have at index five the value one. The remaining values will be zero. We retrieve the training_digits and the training_labels calling mnist. train. next_batch and we specify a batch size of 5, 000. MNIST has 60, 000 images as training data but we're choosing a small batch size so that our code runs quickly. We'll choose a batch of 200 test images in order to test our K-nearest-neighbors algorithm. We'll now set up placeholders to hold our training and test_digits. Notice our training_digits placeholder. It's of kind float, that's for gray scale representation where every pixel value is between zero and 1. 4. The training dataset is a list of images but the shape of this placeholder is none and 784. Before we go into that, let's also instantiate the placeholder for the test digit. We've taken one test digit at a time and compare it with the entire training set. The placeholder for the test_digit is also of type floating point and is of shape 784, a vector with 784 elements. Let's visually understand how the placeholders for the list of images in the training data and a single image in the test data of this particular shape. We've seen earlier that a single image in the MNIST dataset is 28 by 28 having a total of 784 pixels representing our handwritten digits. Now, if you apply a grid pattern on this, where one value in this grid represents the intensity of one pixel. Now, if you take this matrix representation of an image and split it out row wise so that we have one long vector with 784 elements that represents an image as well. We've lost shape information of the image, we don't know its width and its height but all the pixel information is still available to us. That is sufficient for the KNN algorithm, that's why we'll work with the vector of 784 pixels rather than a matrix. Stack each row of the image matrix side by side to get the vector. This is where the 784 pixels come from. It's pretty clear here why the placeholder for the test digit is 784 pixels long. For the training_digits we use a lit of images, so the first dimension which we specify as none, actually indicates the index of each image. We've no idea how many images we are going to pass in which is why we specify none when setting up the placeholder. The number of images is unknown represented by none but the size of each image vector is known which is 784.

Calculating L1 in TensorFlow

Step one is now complete at this point. We're now ready for step two. The next step is to calculate the L1 distance between our test image that's our sample and the entire training set to find the nearest neighbor. This is how the L1 calculation looks like. Don't be intimidated by what you see. I'm going to break this down and visualize each of these steps. Start from the innermost bracket where we have the placeholder for the training_digits and the test digit placeholder. Remember, we have one test digit and a number of training digits that we have to compare. The tf. negative method simply flips the sign for every element in our test_digit placeholder. Let's say we have the pixel vector for one training digit and one test digit. Remember, the training digits are actually an entire series of images, not just one image. The tf. negative method applied to this test digit will simply flip the sign on every pixel value. These are the pixel intensities and they're all negative now. We use the training pixel vector as is. Going back to the same line which calculates L1 Distance, we now use ef. add to add the training_digits with the negative of the test_digits. Every element in the vector representing a training image is added to its corresponding element in the test image. Visually if you add this test vector to this training vector, this is the result that you get for tf. add. Every element in the training vector is added to the corresponding element in the test vector. This will be applied across all vectors in the training dataset. One batch contains 5, 000 images so the test digit will be compared against all those 5, 000 images represented by 5, 000 vectors. Here in this visualization, just imagine that we have three training images. We've just performed the tf. add operation with the test image. We now have a list of vectors where we've added the training digit and the negative of the test digit. We now pass this to tf. abs to calculate the absolute value of every element in the vector. We don't want any negative values here. Running tf. abs on each of these vectors should give us the results that you see on screen. The digits that you see in red are those whose values have changed, whose sign has flipped because we found the absolute value of the vector. Remember that our objective is to find that image that is closest to the test image. We now perform a reduce_sum operation. This sums up all the elements of each vector that we've got so far. Each resultant vector will have just one value and that gives us the absolute distance of that training image from the test image that we are using. Reduce_sum will sum up all the elements of each vector to produce one value. This value is the distance of every training image from our test image. The nearest neighbor for our test image in the training dataset is the one where the distance is minimum. This is because in this particular case of the KNN algorithm, we are working with the value of K equal to one. This we calculate using ef. arg_min. This will give us the index of that image in the training dataset, remember, the training dataset is a list of images which has the minimum distance from the test digit. Running tf. arg_min on this list of images will give us that the image at index is equal to zero is the closest neighbor for our test digit. The label of the image which is at the closest distance from our test digit image is our prediction for the value of the test digit.

Measuring Accuracy

We are now ready to run this algorithm and predict values for our test_digits and measure the accuracy of our algorithm. We have our computation graph. Let's run it and see the accuracy of our prediction using the K-nearest-neighbors algorithm. We'll run a for loop over all our test_digits and then predict each of its values using K-nearest-neighbors. Execute the graph using session. run on our prediction. The inputs to these are the training_digits placeholder and the test_digits placeholder. These are specified in the feed_dictionary. For every iteration of this for loop, we want to take in one test digit and find its value against the entire training dataset. The result of this prediction is the nearest neighbor index specified by nn_index. Use this index to find the label of the training dataset which is the closet to the test data. The predicted label is obtained from training_labels. Notice that we pass in nn_index to get the index of the right training label and the true label is obtained from test_label. This is for the test digit I for the current iteration. Remember that these labels are present in one_hot notation. We use np. argmax to find the index of the one in the vector in one_hot notation. Np. argmax for this vector representing four will give us the index four which is the value of this digit. Now, all that's left is to measure the accuracy of our prediction which we do by comparing our X prediction with our true value of the test digit. And we're done. We are ready to run this program and see how our KNN prediction works. The first thing to notice here that it's downloading and extracting the MNIST data into our local machine. Here are all our prediction and our accuracy for this particular training dataset is 94%, a pretty meaningful outcome. And with this we come to the end of this module and end of this basic course on TensorFlow. In this module you got familiar with the MNIST handwritten dataset and understood the logic behind the K-nearest-neighbors algorithm. You understood what distance measures were and put them altogether to implement K-nearest-neighbors to identify handwritten digits in MNIST. At this point, you should feel ready to tackle bigger and better problems using machine learning in TensorFlow. Thank you and good luck on your learning journey.

Course Overview

Course Overview

Hi, my name is Janani Ravi, and welcome to this course on Understanding the Foundations of Tensorflow. I'll introduce myself. I have a Masters in Electrical Engineering from Stanford, I have worked with companies such as Microsoft, Google, and Flipkart. At Google, I was one of the first engineers working on real time collaborative editing on Google Docs, and I hold four patents for it's underlying technologies. I currently work on my own start-up, Looneycorn, a studio for high quality video content. Tensorflow is slowly growing to be the most popular library for building and deploying machine learning algorithms. This course features the tenents of flow programming language, from very first principles. It starts off with the basics of machine learning, using linear regression as an example, and focuses on understanding fundamentals concepts in Tensorflow and how they apply to machine learning. The concept of a Tensor, the anatomy of a simple program, basic constructs such as constants, variables, place holders, sessions, and the computation draft. The course also introduces TensorBoard, the visualization tool used to view and debug data flow graphs. You'll work with basic math operations and image transformations. You see how common computations are performed. Finally, at the end of this course you will solve a real world machine learning problem, using the M-ness handwritten data set, and the key nearest neighbors algorithm. There are demos for every module to give you hands on experiences with the Tensorflow icon user libraries, for all concepts used in this course. Experience the magic of machine learning by getting started with Tensorflow.

Course author

Author: Janani Ravi	
Janani Ravi
Janani has a Masters degree from Stanford and worked for 7+ years at Google. She was one of the original engineers on Google Docs and holds 4 patents for its real-time collaborative editing...
Course info

Level
Beginner
Rating
4.6 stars with 110 raters(110)
My rating
null stars

Duration
2h 44m
Released
26 Jul 2017
Share course

